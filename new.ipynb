{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5752d94d",
   "metadata": {},
   "source": [
    "## HARPS-N Solar Data Neural Networks Code (TensorFlow 2.x Update)\n",
    "\n",
    "Original by Zoe L. de Beurs \n",
    "Updated to use Tensorflow v2 by Son Do\n",
    "\n",
    "See [de Beurs, Zoe L., Vanderburg, A., Shallue, C.J., et al. (2022)](https://iopscience.iop.org/article/10.3847/1538-3881/ac738e/pdf) for more details \n",
    "\n",
    "*This notebook has been updated to remove deprecated `tf.estimator` calls and use a modern TensorFlow 2.x custom training loop.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tensorflow_addons if needed, as it's used for the optimizer\n",
    "#! pip install tensorflow_addons\n",
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "from types import SimpleNamespace # Used to replace configdict\n",
    "print(tf.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 15\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from astropy.stats import median_absolute_deviation\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/zdebeurs/rv_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rv_net imports.\n",
    "import sys\n",
    "sys.path.append(\"rv_net/\")\n",
    "\n",
    "# Note: We will use the internal definitions for load_dataset_ridge and ridge_regress_harps\n",
    "# The estimator-related imports are no longer needed.\n",
    "# from ops import training\n",
    "# from tf_util import config_util\n",
    "# from tf_util import configdict # Replaced with SimpleNamespace\n",
    "# from tf_util import estimator_runner\n",
    "# from rv_net import data_HARPS_N\n",
    "# from rv_net import  data, rv_model, estimator_util, load_dataset_ridge, ridge_regress_harps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a91f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Residual plotting code\n",
    "import matplotlib as mpl\n",
    "def residual_plot(rv_list, x_axis, ccfs_of_interest, num_ref_ccf, plot_title):\n",
    "    # create color scheme\n",
    "    min_rv = np.min(rv_list)\n",
    "    max_rv = np.max(rv_list)\n",
    "    cscale_residuals = (np.array(rv_list - min_rv) / (max_rv - min_rv))\n",
    "    print(np.min(cscale_residuals), np.max(cscale_residuals))\n",
    "\n",
    "    col = plt.cm.jet([0.25, 0.75])\n",
    "    n = len(ccfs_of_interest)\n",
    "    colors = plt.cm.bwr(cscale_residuals)\n",
    "\n",
    "    # Create the residual plot by looping through the list of CCFs ordered by date\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "    num = 0\n",
    "    for i in np.arange(0, len(ccfs_of_interest)):\n",
    "        if num_ref_ccf == \"median\":\n",
    "            plt.plot(x_axis, ccfs_of_interest[i] - np.median(list(ccfs_of_interest), axis=0), color=colors[num])\n",
    "        else:\n",
    "            if i != num_ref_ccf:\n",
    "                plt.plot(x_axis, ccfs_of_interest[i] - ccfs_of_interest[num_ref_ccf], color=colors[num])\n",
    "        num += 1\n",
    "\n",
    "    plt.title(plot_title)\n",
    "    # make color bar\n",
    "    cmap = mpl.cm.bwr\n",
    "    norm = mpl.colors.Normalize(vmin=(min_rv - np.median(rv_list)), vmax=(max_rv - np.median(rv_list)))\n",
    "    cb = plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax, orientation=\"vertical\", pad=-0.0001)\n",
    "    cb.set_label(label='Stellar Activity Signal (m/s)', size=16, rotation=270, labelpad=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490df5a",
   "metadata": {},
   "source": [
    "## Reading in the data (June 29, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae89e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_method ='cross_val' #\"val\"# \"val\"#\"cross_val\" # \"val\" # \"test\"\n",
    "\n",
    "ccf_len = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869671db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to point towards the location of the files on your device\n",
    "# Assuming the 'Archive_HARPS_N_NEW DRS' directory is in the cloned 'rv_net' repo or at the root\n",
    "!ls 'TF_records_Feb2026'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files for cross-validation\n",
    "# Adjust this path if your data is not inside the rv_net directory\n",
    "DATA_DIR = 'TF_records_Feb2026/' \n",
    "\n",
    "if eval_method ==\"cross_val\":\n",
    "    # Use tf.io.gfile.glob for robust file matching\n",
    "    data_files = tf.io.gfile.glob(DATA_DIR + '*cross_val*')\n",
    "    data_files.sort() # Ensure consistent order\n",
    "    \n",
    "    TRAIN_FILE_NAME_LIST = []\n",
    "    VAL_FILE_NAME_LIST = []\n",
    "\n",
    "    N = len(data_files)\n",
    "    for i in range(N):\n",
    "        val_files = [data_files[i]]\n",
    "        VAL_FILE_NAME_LIST.append(val_files)\n",
    "        train_files = data_files[0:i] + data_files[i+1:]\n",
    "        TRAIN_FILE_NAME_LIST.append(train_files)\n",
    "\n",
    "    NUM_TRAINING_EXAMPLES = 503\n",
    "    NUM_VALIDATION_EXAMPLES = 51\n",
    "\n",
    "elif eval_method ==\"val\":\n",
    "    TRAIN_FILE_NAME_LIST = [[os.path.join(DATA_DIR, \"TF_ccf_full_train\")]]\n",
    "    VAL_FILE_NAME_LIST = [[os.path.join(DATA_DIR, \"TF_ccf_val\")]]#test\")]]\n",
    "\n",
    "    NUM_TRAINING_EXAMPLES = 503\n",
    "    NUM_VALIDATION_EXAMPLES = 61\n",
    "elif eval_method ==\"test\":\n",
    "    TRAIN_FILE_NAME_LIST = [[os.path.join(DATA_DIR, \"TF_ccf_full_train\")]]\n",
    "    VAL_FILE_NAME_LIST = [[os.path.join(DATA_DIR, \"TF_ccf_test\")]]#test\")]]\n",
    "\n",
    "    NUM_TRAINING_EXAMPLES = 503\n",
    "    NUM_VALIDATION_EXAMPLES = 61\n",
    "else:\n",
    "    print(\"Please select a valid evaluation method: 'cross_val' or 'val' or 'test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRAIN_FILE_NAME_LIST)\n",
    "print(VAL_FILE_NAME_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b48ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data loader (Updated)\n",
    "def _example_parser(serialized_example):\n",
    "    \"\"\"Parses a single tf.Example into feature and label tensors.\"\"\"\n",
    "    feature_name = \"Rescaled CCF_residuals_cutoff\"#\"Rescaled CCF_residuals\" #CCF_residuals\n",
    "    label_name = \"activity signal\"#\"RV\",\n",
    "    label2_name = \"BJD\"\n",
    "    data_fields = {\n",
    "        feature_name: tf.io.FixedLenFeature([ccf_len], tf.float32), #[161], tf.float32),\n",
    "        label_name: tf.io.FixedLenFeature([], tf.float32),\n",
    "        label2_name: tf.io.FixedLenFeature([], tf.float32),\n",
    "    }\n",
    "    parsed_fields = tf.io.parse_single_example(serialized_example, features=data_fields)\n",
    "    return parsed_fields[feature_name], parsed_fields[label_name]*1000, parsed_fields[label2_name]\n",
    "\n",
    "\n",
    "def load_dataset(filenames, batch_size, is_training=False):\n",
    "    \"\"\"Loads dataset, updated to remove tf.estimator.ModeKeys.\"\"\"\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    dataset = filename_dataset.flat_map(tf.data.TFRecordDataset)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=NUM_TRAINING_EXAMPLES)\n",
    "    dataset = dataset.map(_example_parser, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce37884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define LinearModel, FCModel, CNNModel\n",
    "# These models are already tf.keras.Model based, so they are TF 2.x compatible!\n",
    "class LinearModel(tf.keras.Model):\n",
    "    \"\"\"A TensorFlow linear regression model.\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"Basic setup.\n",
    "\n",
    "        Args:\n",
    "          hparams: A SimpleNamespace of hyperparameters for building the model.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: If mode is invalid.\n",
    "        \"\"\"\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        #self.weights = tf.Variable(tf.zeros(self.hparams.num_features))\n",
    "        self.dense_layer = tf.keras.layers.Dense(\n",
    "            1, kernel_initializer=tf.zeros_initializer, use_bias=False)\n",
    "        \n",
    "    def call(self, features, training=False):\n",
    "      # return tf.tensordot(features, self.weights, axes=1)\n",
    "      return tf.squeeze(self.dense_layer(features), axis=-1) # Squeeze last axis\n",
    "\n",
    "class FCModel(tf.keras.Model):\n",
    "    \"\"\"A TensorFlow linear regression model.\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"Basic setup.\n",
    "\n",
    "        Args:\n",
    "          hparams: A SimpleNamespace of hyperparameters for building the model.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: If mode is invalid.\n",
    "        \"\"\"\n",
    "        super(FCModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        #self.hidden_layer1 = tf.keras.layers.Dense(\n",
    "        #    self.hparams.num_dense_units, activation=tf.keras.activations.relu)\n",
    "        self.dense_layers = [\n",
    "          tf.keras.layers.Dense(\n",
    "              hparams.num_dense_units,\n",
    "              activation=tf.keras.activations.relu)\n",
    "          for i in range(hparams.num_dense_layers)\n",
    "        ]\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, training=False):\n",
    "        net = tf.expand_dims(features, -1)\n",
    "        batch_size, length, depth = net.shape\n",
    "        net = tf.reshape(net, [batch_size, length*depth])\n",
    "        for dense in self.dense_layers:\n",
    "            net = dense(net)\n",
    "        net = self.output_layer(net)\n",
    "        return tf.squeeze(net, axis=-1) # Squeeze last axis\n",
    "\n",
    "# @title Define RVLinearModel\n",
    "class CNNModel(tf.keras.Model):\n",
    "    \"\"\"A TensorFlow linear regression model.\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"Basic setup.\n",
    "\n",
    "        Args:\n",
    "          hparams: A SimpleNamespace of hyperparameters for building the model.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: If mode is invalid.\n",
    "        \"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.conv_layers = [\n",
    "          tf.keras.layers.Conv1D(\n",
    "              filters=hparams.num_conv_filters, \n",
    "              kernel_size=hparams.conv_kernel_size,\n",
    "              activation=tf.keras.activations.relu,\n",
    "              padding=\"same\")\n",
    "          for i in range(hparams.num_conv_layers)\n",
    "        ]\n",
    "        self.dense_layers = [\n",
    "          tf.keras.layers.Dense(\n",
    "              hparams.num_dense_units,\n",
    "              activation=tf.keras.activations.relu)\n",
    "          for i in range(hparams.num_dense_layers)\n",
    "        ]\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, training=False):\n",
    "        net = tf.expand_dims(features, -1)\n",
    "        for conv in self.conv_layers:\n",
    "            net = conv(net)\n",
    "        batch_size, length, depth = net.shape\n",
    "        net = tf.reshape(net, [batch_size, length*depth])\n",
    "        for dense in self.dense_layers:\n",
    "            net = dense(net)\n",
    "        net = self.output_layer(net)\n",
    "        return tf.squeeze(net, axis=-1) # Squeeze last axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Custom train loop (Updated)\n",
    "\n",
    "def make_predictions(model, dataset):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_bjds = []\n",
    "    for features, labels, bjds in dataset:\n",
    "        preds = model(features, training=False)\n",
    "        all_preds.append(preds.numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        all_bjds.append(bjds.numpy())\n",
    "    return np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_bjds)\n",
    "\n",
    "def train(model, hparams, plots, model_name, num_epochs):#=100):\n",
    "    # Updated load_dataset calls\n",
    "    train_dataset = load_dataset(TRAIN_FILE_NAME, batch_size=hparams.batch_size, is_training=True)\n",
    "    val_dataset = load_dataset(VAL_FILE_NAME, batch_size=min(1024, NUM_VALIDATION_EXAMPLES), is_training=False)\n",
    "    \n",
    "    loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    \n",
    "    # Use Adam optimizer as a robust default, SGD+Momentum is also fine.\n",
    "    # Opt = tfa.optimizers.extend_with_decoupled_weight_decay(tf.optimizers.SGD)\n",
    "    # optimizer = Opt(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate, momentum=hparams.momentum)\n",
    "\n",
    "    # Using AdamW (Adam with decoupled weight decay)\n",
    "    optimizer = tf.keras.optimizers.AdamW(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate)\n",
    "\n",
    "    metrics = [\n",
    "             tf.keras.metrics.MeanSquaredError(\"train_loss\"),\n",
    "             tf.keras.metrics.RootMeanSquaredError(\"train_rmse\")\n",
    "    ]\n",
    "    weight_decay_list_t.append(hparams.weight_decay)\n",
    "    gaussian_noise_list_t.append(hparams.gaussian_noise_scale)\n",
    "\n",
    "    metric_values = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Reset metric values for each new epoch.\n",
    "        for m in metrics:\n",
    "            m.reset_state()\n",
    "\n",
    "        # Train over all batches in the training set.\n",
    "        for features, labels, bjds in train_dataset:\n",
    "            if hparams.gaussian_noise_scale:\n",
    "                features += tf.random.normal(features.shape, stddev=hparams.gaussian_noise_scale)\n",
    "                #print(hparams.gaussian_noise_scale)\n",
    "            # One training step.\n",
    "            with tf.GradientTape() as t:\n",
    "                preds = model(features, training=True)\n",
    "                loss = loss_fn(labels, preds)\n",
    "            grads = t.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            # Update the metrics.\n",
    "            for m in metrics:\n",
    "                m(labels, preds)\n",
    "    \n",
    "        # End of an epoch.\n",
    "        epoch_metrics = {\"epoch\": epoch}\n",
    "        # First, log the training metrics.\n",
    "        for m in metrics:\n",
    "            epoch_metrics[m.name] = m.result().numpy()\n",
    "        # Next, evaluate over the validation set.\n",
    "        labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "        epoch_metrics[\"val_rmse\"] = np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "        # Add a metric for the raw scatter started with\n",
    "        epoch_metrics[\"original_rmse\"] = np.std(labels_val)\n",
    "        # Add a metric for raw scatter - corrected scatter\n",
    "        epoch_metrics[\"difference_rmse\"] = np.std(labels_val) - np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "        \n",
    "        # Log metrics to tensorboard.\n",
    "        # NOTE: This requires a tf.summary.SummaryWriter setup. Commenting out for simplicity.\n",
    "        # for metric, value in epoch_metrics.items():\n",
    "        #    tf.summary.scalar(metric, value, step=epoch)\n",
    "        \n",
    "        epoch_metrics[\"epoch\"] = epoch\n",
    "        # Print metric values at selected epochs.\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            print(\"{epoch}: Train loss: {train_loss:.4}, Train RMSE: {train_rmse:.4}, Val RMSE: {val_rmse:.4}\".format(**epoch_metrics))\n",
    "        metric_values.append(epoch_metrics)\n",
    "\n",
    "    # Gather predictions\n",
    "    labels, preds, bjd = make_predictions(model, train_dataset)\n",
    "    labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "    all_bjds_val.append(bjd_val)\n",
    "    bjd_run_val.append(bjd_val)\n",
    "    all_pred_val.append(preds_val)\n",
    "    pred_run_val.append(preds_val)\n",
    "    all_labels_val.append(labels_val)\n",
    "    labels_run_val.append(labels_val)\n",
    "\n",
    "    # Scatter reduction plot\n",
    "    sd_x = np.std(labels_val, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(labels_val - preds_val)))\n",
    "    rms_x_list.append(rms_x)\n",
    "    rms_avg_list.append(rms_x)\n",
    "    stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "    x_range = np.linspace(-4,5.5, 17)\n",
    "    upper_bound = x_range+rms_x\n",
    "    lower_bound = x_range-rms_x\n",
    "\n",
    "    if plots==\"ON\":\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(24, 6))\n",
    "        ax = axes[0]\n",
    "        ax.plot([m[\"train_rmse\"] for m in metric_values], label=\"Train RMSE\")\n",
    "        ax.plot([m[\"val_rmse\"] for m in metric_values], label=\"Validation RMSE\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "        # Gather predictions to plot against labels.\n",
    "        ax = axes[1]\n",
    "        ax.plot(preds, labels, \".\", label=\"Training\")\n",
    "        ax.plot(preds_val, labels_val, \".\", label=\"Validation\")\n",
    "        ax.set_xlabel(\"Actual Y\")\n",
    "        ax.set_ylabel(\"Predicted Y\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # plot the scatter reduction plot\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        ax1.plot(labels_val, preds_val, \".\")\n",
    "        ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "        #ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "        rms_fill=rms_x#0.15\n",
    "        ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                    alpha=0.5, label=\"1 standard deviation\")\n",
    "        ax1.set_xlim(-4, 4);\n",
    "        ax1.set_ylim(-4, 4);\n",
    "        ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "        ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "        ax1.set_title(model_name+\" Model Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "        #hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "        textstr = '\\n'.join((\n",
    "            r'Raw scatter=%.3f m/s' % (sd_x, ),\n",
    "            r'Corrected scatter=%.3f m/s' % (rms_x, ),\n",
    "            r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "        ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "            ha=\"left\", va=\"top\",\n",
    "            bbox=dict(boxstyle=\"square\",\n",
    "                      ec=(1., 0.5, 0.5),\n",
    "                      fc=(1., 0.8, 0.8),\n",
    "                      ))\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "    else:\n",
    "        textstr = '\\n'.join((\n",
    "            r'Raw scatter=%.3f m/s' % (sd_x, ),\n",
    "            r'Corrected scatter=%.3f m/s' % (rms_x, ),\n",
    "            r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "        print(textstr)\n",
    "    \n",
    "    return metric_values\n",
    "  \n",
    "\n",
    "def train_no_logging(model, hparams, plots, model_name, num_epochs, TRAIN_FILE_NAME, VAL_FILE_NAME):#=100):\n",
    "    # Updated load_dataset calls\n",
    "    train_dataset = load_dataset(TRAIN_FILE_NAME, batch_size=hparams.batch_size, is_training=True)\n",
    "    val_dataset = load_dataset(VAL_FILE_NAME, batch_size=min(1024, NUM_VALIDATION_EXAMPLES), is_training=False)\n",
    "    \n",
    "    loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    \n",
    "    # Use Adam optimizer as a robust default, SGD+Momentum is also fine.\n",
    "    # Opt = tfa.optimizers.extend_with_decoupled_weight_decay(tf.optimizers.SGD)\n",
    "    # optimizer = Opt(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate, momentum=hparams.momentum)\n",
    "    \n",
    "    # Using AdamW (Adam with decoupled weight decay)\n",
    "    optimizer = tf.keras.optimizers.AdamW(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate)\n",
    "\n",
    "    metrics = [\n",
    "             tf.keras.metrics.MeanSquaredError(\"train_loss\"),\n",
    "             tf.keras.metrics.RootMeanSquaredError(\"train_rmse\")\n",
    "    ]\n",
    "    weight_decay_list_t.append(hparams.weight_decay)\n",
    "    gaussian_noise_list_t.append(hparams.gaussian_noise_scale)\n",
    "\n",
    "    metric_values = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Reset metric values for each new epoch.\n",
    "        for m in metrics:\n",
    "            m.reset_state()\n",
    "\n",
    "        # Train over all batches in the training set.\n",
    "        for features, labels, bjds in train_dataset:\n",
    "            if hparams.gaussian_noise_scale:\n",
    "                features += tf.random.normal(features.shape, stddev=hparams.gaussian_noise_scale)\n",
    "                #print(hparams.gaussian_noise_scale)\n",
    "            # One training step.\n",
    "            with tf.GradientTape() as t:\n",
    "                preds = model(features, training=True)\n",
    "                loss = loss_fn(labels, preds)\n",
    "            grads = t.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            # Update the metrics.\n",
    "            for m in metrics:\n",
    "                m(labels, preds)\n",
    "    \n",
    "        # End of an epoch.\n",
    "        epoch_metrics = {\"epoch\": epoch}\n",
    "        # First, log the training metrics.\n",
    "        for m in metrics:\n",
    "            epoch_metrics[m.name] = m.result().numpy()\n",
    "        # Next, evaluate over the validation set.\n",
    "        labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "        epoch_metrics[\"val_rmse\"] = np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "        # Add a metric for the raw scatter started with\n",
    "        epoch_metrics[\"original_rmse\"] = np.std(labels_val)\n",
    "        # Add a metric for raw scatter - corrected scatter\n",
    "        epoch_metrics[\"difference_rmse\"] = np.std(labels_val) - np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "        \n",
    "        # Log metrics to tensorboard.\n",
    "        # NOTE: This requires a tf.summary.SummaryWriter setup. Commenting out for simplicity.\n",
    "        # for metric, value in epoch_metrics.items():\n",
    "        #    tf.summary.scalar(metric, value, step=epoch)\n",
    "        \n",
    "        epoch_metrics[\"epoch\"] = epoch\n",
    "        # Print metric values at selected epochs.\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            print(\"{epoch}: Train loss: {train_loss:.4}, Train RMSE: {train_rmse:.4}, Val RMSE: {val_rmse:.4}\".format(**epoch_metrics))\n",
    "        metric_values.append(epoch_metrics)\n",
    "\n",
    "    # Gather predictions\n",
    "    labels, preds, bjd = make_predictions(model, train_dataset)\n",
    "    labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "    all_bjds_val.append(bjd_val)\n",
    "    bjd_run_val.append(bjd_val)\n",
    "    all_pred_val.append(preds_val)\n",
    "    pred_run_val.append(preds_val)\n",
    "    all_labels_val.append(labels_val)\n",
    "    labels_run_val.append(labels_val)\n",
    "\n",
    "    # Scatter reduction plot\n",
    "    sd_x = np.std(labels_val, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(labels_val - preds_val)))\n",
    "    rms_x_list.append(rms_x)\n",
    "    rms_avg_list.append(rms_x)\n",
    "    stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "    x_range = np.linspace(-4,5.5, 17)\n",
    "    upper_bound = x_range+rms_x\n",
    "    lower_bound = x_range-rms_x\n",
    "\n",
    "    if plots==\"ON\":\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(24, 6))\n",
    "        ax = axes[0]\n",
    "        ax.plot([m[\"train_rmse\"] for m in metric_values], label=\"Train RMSE\")\n",
    "        ax.plot([m[\"val_rmse\"] for m in metric_values], label=\"Validation RMSE\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "        # Gather predictions to plot against labels.\n",
    "        ax = axes[1]\n",
    "        ax.plot(preds, labels, \".\", label=\"Training\")\n",
    "        ax.plot(preds_val, labels_val, \".\", label=\"Validation\")\n",
    "        ax.set_xlabel(\"Actual Y\")\n",
    "        ax.set_ylabel(\"Predicted Y\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # plot the scatter reduction plot\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        ax1.plot(labels_val, preds_val, \".\")\n",
    "        ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "        #ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "        rms_fill=rms_x#0.15\n",
    "        ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                        alpha=0.5, label=\"1 standard deviation\")\n",
    "        ax1.set_xlim(-4, 4);\n",
    "        ax1.set_ylim(-4, 4);\n",
    "        ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "        ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "        ax1.set_title(model_name+\" Model Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "        #hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "        textstr = '\\n'.join((\n",
    "            r'Raw scatter=%.3f m/s' % (sd_x, ),\n",
    "            r'Corrected scatter=%.3f m/s' % (rms_x, ),\n",
    "            r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "        ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "                ha=\"left\", va=\"top\",\n",
    "                bbox=dict(boxstyle=\"square\",\n",
    "                          ec=(1., 0.5, 0.5),\n",
    "                          fc=(1., 0.8, 0.8),\n",
    "                          ))\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "    else:\n",
    "        textstr = '\\n'.join((\n",
    "            r'Raw scatter=%.3f m/s' % (sd_x, ),\n",
    "            r'Corrected scatter=%.3f m/s' % (rms_x, ),\n",
    "            r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "        print(textstr)\n",
    "    \n",
    "    return metric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e12044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title train for cross-val (v2)\n",
    "# This version calls train_no_logging, so it's already updated.\n",
    "\n",
    "# Re-define make_predictions to avoid issues with cell execution order\n",
    "def make_predictions(model, dataset):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_bjds = []\n",
    "    for features, labels, bjds in dataset:\n",
    "        preds = model(features, training=False)\n",
    "        all_preds.append(preds.numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        all_bjds.append(bjds.numpy())\n",
    "    return np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_bjds)\n",
    "\n",
    "def train_cross_val_v2(model_builder, hparams, plots, model_name, num_epochs):#=100):\n",
    "    metrics_list = []\n",
    "    global bjd_run_val, pred_run_val, labels_run_val # Make sure these are accessible\n",
    "    \n",
    "    for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "        #print(\"Model {}. Learning_rate: {:.5f}. Dense units: {}. Dense layers {}. Weight decay: {:.5f}\".format(\n",
    "        #  model_num, hparams.learning_rate, hparams.num_dense_units, hparams.num_dense_layers, hparams.weight_decay))\n",
    "        print(f\"Model: {model_name}, Cross-val number: {index+1}\")\n",
    "        print(f\"Learning_rate: {hparams.learning_rate:.5f}, Weight decay: {hparams.weight_decay:.5f}\")\n",
    "\n",
    "        # Re-initialize model for each cross-val fold\n",
    "        model = model_builder(hparams)\n",
    "\n",
    "        global TRAIN_FILE_NAME, VAL_FILE_NAME\n",
    "        TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "        VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "        print(TRAIN_FILE_NAME)\n",
    "        print(VAL_FILE_NAME)\n",
    "        metrics = [\n",
    "             tf.keras.metrics.MeanSquaredError(\"train_loss\"),\n",
    "             tf.keras.metrics.RootMeanSquaredError(\"train_rmse\")\n",
    "        ]\n",
    "\n",
    "        metric_values = train_no_logging(model, hparams, plots, model_name, num_epochs, TRAIN_FILE_NAME, VAL_FILE_NAME)\n",
    "        metrics_list.append(metric_values[-1])\n",
    "\n",
    "    crossval_metrics = {\"index\": index}\n",
    "    # compute median training metrices\n",
    "    for m in metrics:\n",
    "        crossval_metrics[m.name] = np.median([z[m.name] for z in metrics_list])\n",
    "    # compute median val, original and difference rmse\n",
    "    crossval_metrics['val_rmse'] = np.mean([z['val_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['original_rmse'] = np.mean([z['original_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['difference_rmse'] = np.mean([z['difference_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['epoch'] = np.mean([z['epoch'] for z in metrics_list])\n",
    "    print(\"overall val_rmse: \"+str(crossval_metrics['val_rmse']))\n",
    "    print(\"_____________________________________________________\")\n",
    "    \n",
    "    # Log metrics to tensorboard.\n",
    "    # NOTE: This requires a tf.summary.SummaryWriter setup. Commenting out for simplicity.\n",
    "    # for metric, value in crossval_metrics.items():\n",
    "    #    tf.summary.scalar(metric, value, step=crossval_metrics['epoch'])\n",
    "\n",
    "    # Check if pred_run_val is populated before processing\n",
    "    if pred_run_val:\n",
    "        mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "        mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "        mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "\n",
    "        all_mean_val_preds.append(mean_val_preds)\n",
    "        all_mean_val_labels.append(mean_val_labels)\n",
    "        all_mean_val_bjds.append(mean_val_bjds)\n",
    "    else:\n",
    "        print(\"Warning: pred_run_val was empty for this fold.\")\n",
    "\n",
    "    all_pred_val.append(pred_run_val)\n",
    "    all_labels_val.append(labels_run_val)\n",
    "    all_bjds_val.append(bjd_run_val)\n",
    "\n",
    "    return crossval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title train for cross-val (v1) (Updated)\n",
    "# This version has its own training loop, which also needs updating.\n",
    "\n",
    "# Re-define make_predictions to avoid issues with cell execution order\n",
    "def make_predictions(model, dataset):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_bjds = []\n",
    "    for features, labels, bjds in dataset:\n",
    "        preds = model(features, training=False)\n",
    "        all_preds.append(preds.numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        all_bjds.append(bjds.numpy())\n",
    "    return np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_bjds)\n",
    "\n",
    "def train_cross_val_v1(model_builder, hparams, plots, model_name, num_epochs):#=100):\n",
    "    metrics_list = []\n",
    "    for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "        #print(\"Model {}. Learning_rate: {:.5f}. Dense units: {}. Dense layers {}. Weight decay: {:.5f}\".format(\n",
    "        #    model_num, hparams.learning_rate, hparams.num_dense_units, hparams.num_dense_layers, hparams.weight_decay))\n",
    "        print(f\"Model: {model_name}, Cross-val number: {index+1}\")\n",
    "        print(f\"Learning_rate: {hparams.learning_rate:.5f}, Weight decay: {hparams.weight_decay:.5f}\")\n",
    "\n",
    "        # Re-initialize model for each cross-val fold\n",
    "        model = model_builder(hparams)\n",
    "\n",
    "        global TRAIN_FILE_NAME, VAL_FILE_NAME\n",
    "        TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "        VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "        \n",
    "        # Updated load_dataset calls\n",
    "        train_dataset = load_dataset(TRAIN_FILE_NAME, batch_size=hparams.batch_size, is_training=True)\n",
    "        val_dataset = load_dataset(VAL_FILE_NAME, batch_size=min(1024, NUM_VALIDATION_EXAMPLES), is_training=False)\n",
    "        \n",
    "        loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "        \n",
    "        # Use Adam optimizer as a robust default, SGD+Momentum is also fine.\n",
    "        # Opt = tfa.optimizers.extend_with_decoupled_weight_decay(tf.optimizers.SGD)\n",
    "        # optimizer = Opt(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate, momentum=hparams.momentum)\n",
    "        \n",
    "        # Using AdamW (Adam with decoupled weight decay)\n",
    "        optimizer = tf.keras.optimizers.AdamW(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate)\n",
    "\n",
    "        metrics = [\n",
    "              tf.keras.metrics.MeanSquaredError(\"train_loss\"),\n",
    "              tf.keras.metrics.RootMeanSquaredError(\"train_rmse\")\n",
    "        ]\n",
    "        weight_decay_list_t.append(hparams.weight_decay)\n",
    "        gaussian_noise_list_t.append(hparams.gaussian_noise_scale)\n",
    "\n",
    "        # Reset metric values for each new cross-val slice.\n",
    "        for m in metrics:\n",
    "            m.reset_state()\n",
    "\n",
    "        metric_values = []\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            # Reset metric values for each new epoch.\n",
    "            for m in metrics:\n",
    "                m.reset_state()\n",
    "\n",
    "            # Train over all batches in the training set.\n",
    "            for features, labels, bjds in train_dataset:\n",
    "                if hparams.gaussian_noise_scale:\n",
    "                    features += tf.random.normal(features.shape, stddev=hparams.gaussian_noise_scale)\n",
    "                    #print(hparams.gaussian_noise_scale)\n",
    "                # One training step.\n",
    "                with tf.GradientTape() as t:\n",
    "                    preds = model(features, training=True)\n",
    "                    loss = loss_fn(labels, preds)\n",
    "                grads = t.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "                # Update the metrics.\n",
    "                for m in metrics:\n",
    "                    m(labels, preds)\n",
    "      \n",
    "            # End of an epoch.\n",
    "            epoch_metrics = {\"epoch\": epoch}\n",
    "            # First, log the training metrics.\n",
    "            for m in metrics:\n",
    "                epoch_metrics[m.name] = m.result().numpy()\n",
    "            # Next, evaluate over the validation set.\n",
    "            labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "            epoch_metrics[\"val_rmse\"] = np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "            # Add a metric for the raw scatter started with\n",
    "            epoch_metrics[\"original_rmse\"] = np.std(labels_val)\n",
    "            # Add a metric for raw scatter - corrected scatter\n",
    "            epoch_metrics[\"difference_rmse\"] = np.std(labels_val) - np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "            \n",
    "            # Log metrics to tensorboard.\n",
    "            # NOTE: This requires a tf.summary.SummaryWriter setup. Commenting out for simplicity.\n",
    "            #for metric, value in epoch_metrics.items():\n",
    "            #  tf.summary.scalar(metric, value, step=epoch)\n",
    "            \n",
    "            epoch_metrics[\"epoch\"] = epoch\n",
    "            # Print metric values at selected epochs.\n",
    "            if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "                print(\"{epoch}: Train loss: {train_loss:.4}, Train RMSE: {train_rmse:.4}, Val RMSE: {val_rmse:.4}\".format(**epoch_metrics))\n",
    "            metric_values.append(epoch_metrics)\n",
    "        metrics_list.append(metric_values[-1])\n",
    "\n",
    "    crossval_metrics = {\"index\": index}\n",
    "    # compute median training metrices\n",
    "    for m in metrics:\n",
    "        crossval_metrics[m.name] = np.median([z[m.name] for z in metrics_list])\n",
    "    # compute median val, original and difference rmse\n",
    "    crossval_metrics['val_rmse'] = np.mean([z['val_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['original_rmse'] = np.median([z['original_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['difference_rmse'] = np.median([z['difference_rmse'] for z in metrics_list])\n",
    "    \n",
    "    # Log metrics to tensorboard.\n",
    "    # NOTE: This requires a tf.summary.SummaryWriter setup. Commenting out for simplicity.\n",
    "    # for metric, value in crossval_metrics.items():\n",
    "    #    tf.summary.scalar(metric, value, step=epoch) \n",
    "\n",
    "    return crossval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cece5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Linear NN Model run (Updated)\n",
    "\n",
    "print(\"Starting Linear NN Model Run...\")\n",
    "\n",
    "rms_avg_list = []\n",
    "weight_decay_list_t = []\n",
    "gaussian_noise_list_t = []\n",
    "rms_x_list = []\n",
    "\n",
    "all_bjds_val = []\n",
    "all_pred_val = []\n",
    "all_labels_val = []\n",
    "all_mean_val_preds = []\n",
    "all_mean_val_bjds = []\n",
    "all_mean_val_labels = []\n",
    "\n",
    "for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "    TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "    VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "    bjd_run_val = []\n",
    "    pred_run_val = []\n",
    "    labels_run_val = []\n",
    "    all_bjds_val = [] # Re-init for this fold\n",
    "    \n",
    "    for k in range(0,1):#10): #should be 10 for a full run\n",
    "        # Updated to use SimpleNamespace\n",
    "        hparams = SimpleNamespace(**dict(\n",
    "          num_features=ccf_len, # Use ccf_len variable\n",
    "          learning_rate=1e-3,\n",
    "          momentum=0.9, # Note: AdamW is used in train(), momentum is not used by it\n",
    "          batch_size=1024,\n",
    "          weight_decay=1e-5, \n",
    "          gaussian_noise_scale=0, \n",
    "        ))\n",
    "        model = LinearModel(hparams)\n",
    "        train(model, hparams, plots=\"ON\", model_name = \"Linear\", num_epochs= 1000) # 45)\n",
    "        print(model)\n",
    "        print(\"________________________\")\n",
    "        print(\"Cross-val number: \"+str(index+1)+\", Run number: \"+str(k+1))\n",
    "\n",
    "    if pred_run_val: # Only calculate mean if list is not empty\n",
    "        mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "        mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "        mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "        all_mean_val_preds.append(mean_val_preds.tolist())\n",
    "        all_mean_val_labels.append(mean_val_labels.tolist())\n",
    "        all_mean_val_bjds.append(mean_val_bjds.tolist())\n",
    "    else:\n",
    "        print(f\"Warning: No predictions recorded for fold {index+1}\")\n",
    "\n",
    "avg = np.mean(rms_avg_list)\n",
    "print(\"________________________\")\n",
    "print(f\"Final Average RMS: {avg}\")\n",
    "\n",
    "#flatten the lists\n",
    "all_mean_val_preds_flat = [item for sublist in all_mean_val_preds for item in sublist]\n",
    "all_mean_val_labels_flat = [item for sublist in all_mean_val_labels for item in sublist]\n",
    "all_mean_val_bjds_flat = [item for sublist in all_mean_val_bjds for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_mean_val_labels_flat: # Only run if predictions were made\n",
    "    df = pd.DataFrame(list(zip(all_mean_val_labels_flat, all_mean_val_preds_flat, all_mean_val_bjds_flat)), \n",
    "                   columns =['labels', 'preds', 'BJD']) \n",
    "    df_sorted = df.sort_values(by=['BJD'])\n",
    "    #df_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/cross_val_preds/val_preds_linearNN_06_30_2021.xlsx', index = False)\n",
    "\n",
    "    # calculation of scatter removed\n",
    "    from astropy.stats import median_absolute_deviation\n",
    "\n",
    "    #df_sorted = df_from_excel\n",
    "    labels_68_percent_by_2 = (np.percentile(df_sorted[\"labels\"], 84)-np.percentile(df_sorted[\"labels\"], 16))/2\n",
    "    labels_median_absolute_deviation_norm = median_absolute_deviation(df_sorted[\"labels\"])*1.4826\n",
    "    preds_68_percent_by_2 = (np.percentile(df_sorted[\"preds\"], 84)-np.percentile(df_sorted[\"preds\"], 16))/2\n",
    "\n",
    "    corrected_rvs = df_sorted[\"labels\"]-df_sorted[\"preds\"]\n",
    "    corrected_rvs_68_percent_by_2 = (np.percentile(corrected_rvs, 84)-np.percentile(corrected_rvs, 16))/2\n",
    "    corrected_rvs_median_absolute_deviation_norm = median_absolute_deviation(corrected_rvs)*1.4826\n",
    "\n",
    "    sd_labels = np.std(df_sorted[\"labels\"], ddof=1)\n",
    "    sd_corrected_rv = np.std(corrected_rvs, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(df_sorted[\"labels\"] - df_sorted[\"preds\"])))\n",
    "\n",
    "\n",
    "    print(\"labels: sd: \"+str(sd_labels))\n",
    "    print(\"corrected rv: sd: \"+str(sd_corrected_rv))\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"labels: MAD*1.4826: \"+str(labels_median_absolute_deviation_norm))\n",
    "    print(\"corrected rv: MAD*1.4826: \"+str(corrected_rvs_median_absolute_deviation_norm))\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"labels: 68 percent/2: \"+str(labels_68_percent_by_2))\n",
    "    print(\"corrected rv: 68 percent/2: \"+str(corrected_rvs_68_percent_by_2))\n",
    "\n",
    "\n",
    "    # plot the average results\n",
    "    # Scatter reduction plot\n",
    "    sd_x = np.std(all_mean_val_labels_flat, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(all_mean_val_labels_flat - np.array(all_mean_val_preds_flat))))\n",
    "    # rms_x_list.append(rms_x) # This list is from the training loop, may be confusing here\n",
    "    # rms_avg_list.append(rms_x)\n",
    "    stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "    x_range = np.linspace(-4,5.5, 17)\n",
    "    upper_bound = x_range+rms_x\n",
    "    lower_bound = x_range-rms_x\n",
    "\n",
    "    # plot the scatter reduction plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(21, 6))\n",
    "    ax1 = ax[0]\n",
    "    ax1.plot(df_sorted[\"labels\"], df_sorted[\"preds\"], \".\")\n",
    "    ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "    #ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "    rms_fill=rms_x#0.15\n",
    "    ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                    alpha=0.5, label=\"1 standard deviation\")\n",
    "    ax1.set_xlim(-4, 5);\n",
    "    ax1.set_ylim(-4, 5);\n",
    "    ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "    ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "    ax1.set_title(\"Linear NN Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "    #hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "    textstr = '\\n'.join((\n",
    "        r'Raw scatter=%.3f m/s' % (labels_68_percent_by_2, ),\n",
    "        r'Corrected scatter=%.3f m/s' % (corrected_rvs_68_percent_by_2, ),\n",
    "        r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "    ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "            ha=\"left\", va=\"top\",\n",
    "            bbox=dict(facecolor='#fdcf44',edgecolor='k',\n",
    "                      boxstyle=\"square\",\n",
    "                      #ec=(1., 0.5, 0.5),\n",
    "                      #fc=(1., 0.8, 0.8),\n",
    "                      ))\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot predictions over time\n",
    "    ax2 = ax[1]\n",
    "    ax2.plot(df_sorted[\"BJD\"], df_sorted[\"labels\"], \".\",color='k',\n",
    "             markersize=10,label=\"labels\")\n",
    "    ax2.plot(df_sorted[\"BJD\"], df_sorted[\"preds\"], \".\", markersize=10,\n",
    "             label=\"preds\")\n",
    "    ax2.set_xlabel(\"Time (BJD)\")\n",
    "    ax2.set_ylabel(\"RV (m/s)\")\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "else:\n",
    "    print(\"Skipping Linear NN plots because no predictions were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04396c38",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title DatasetBuilder for Ridge (Updated)\n",
    "class DatasetBuilder(object):\n",
    "    \"\"\"Dataset builder class.\"\"\"\n",
    "\n",
    "    def __init__(self, file_pattern, hparams, is_training, ccf_len, repeat=1):\n",
    "        \"\"\"Initializes the dataset builder.\n",
    "        Args:\n",
    "          file_pattern: File pattern matching input file shards, e.g.\n",
    "            \"/tmp/train-?????-of-00100\".\n",
    "          hparams: A SimpleNamespace.\n",
    "          is_training: Boolean, whether to shuffle.\n",
    "          ccf_len: Integer, length of the CCF feature.\n",
    "          repeat: The number of times to repeat the dataset. If None, the dataset\n",
    "            will repeat indefinitely.\n",
    "        \"\"\"\n",
    "        self.file_pattern = file_pattern\n",
    "        self.hparams = hparams\n",
    "        self.is_training = is_training\n",
    "        self.repeat = repeat\n",
    "        self.ccf_len = ccf_len\n",
    "\n",
    "    def __call__(self):\n",
    "        is_training = self.is_training\n",
    "\n",
    "        # Dataset of file names.\n",
    "        filename_dataset = tf.data.Dataset.list_files(self.file_pattern,\n",
    "                                                      shuffle=is_training)\n",
    "\n",
    "        # Dataset of serialized tf.Examples.\n",
    "        dataset = filename_dataset.flat_map(tf.data.TFRecordDataset)\n",
    "\n",
    "        # Shuffle in training mode.\n",
    "        if is_training:\n",
    "            dataset = dataset.shuffle(self.hparams.shuffle_values_buffer)\n",
    "\n",
    "        # Possibly repeat.\n",
    "        if self.repeat != 1:\n",
    "            dataset = dataset.repeat(self.repeat)\n",
    "\n",
    "        def _example_parser(serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into feature and label tensors.\"\"\"\n",
    "            data_fields = {\n",
    "                self.hparams.ccf_feature_name: tf.io.FixedLenFeature([self.ccf_len], tf.float32),\n",
    "                self.hparams.label_feature_name: tf.io.FixedLenFeature([], tf.float32),\n",
    "                self.hparams.label_feature_name2: tf.io.FixedLenFeature([], tf.float32),\n",
    "            }\n",
    "            parsed_fields = tf.io.parse_single_example(serialized_example, features=data_fields)\n",
    "            ccf_data = parsed_fields[self.hparams.ccf_feature_name]\n",
    "            label = parsed_fields[self.hparams.label_feature_name]\n",
    "            label *= self.hparams.label_rescale_factor  # Rescale the label.\n",
    "            label2 = parsed_fields[self.hparams.label_feature_name2]\n",
    "            return {\n",
    "                \"ccf_data\": ccf_data,\n",
    "                \"label\": label,\n",
    "                \"bjd\": label2,\n",
    "            }\n",
    "\n",
    "        # Map the parser over the dataset.\n",
    "        dataset = dataset.map(_example_parser, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        # Batch results by up to batch_size.\n",
    "        dataset = dataset.batch(self.hparams.batch_size)\n",
    "\n",
    "        # Prefetch a few batches.\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title load_dataset_ridge (Updated)\n",
    "def load_dataset_ridge(filename, ccf_len):\n",
    "    dataset_hparams = SimpleNamespace(**dict(\n",
    "        ccf_feature_name='Rescaled CCF_residuals_cutoff',#\"Rescaled CCF_residuals\", #CCF_residuals\n",
    "        label_feature_name= \"activity signal\",#\"RV\",\n",
    "        label_feature_name2= \"BJD\",\n",
    "        batch_size=300,\n",
    "        label_rescale_factor=1000,\n",
    "        shuffle_values_buffer=1000, # Added for completeness, though not used in eval\n",
    "      ))\n",
    "    # Updated DatasetBuilder call\n",
    "    dataset = DatasetBuilder(filename, dataset_hparams, is_training=False, ccf_len=ccf_len)()\n",
    "    \n",
    "    ccf_data_list, labels_list, bjds_list = [], [], []\n",
    "    for batch in dataset:\n",
    "        ccf_data_list.append(batch[\"ccf_data\"].numpy())\n",
    "        labels_list.append(batch[\"label\"].numpy())\n",
    "        bjds_list.append(batch[\"bjd\"].numpy())\n",
    "\n",
    "    if not ccf_data_list:\n",
    "        print(f\"Warning: No data loaded from {filename}\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    ccf_data = np.concatenate(ccf_data_list)\n",
    "    labels = np.concatenate(labels_list)\n",
    "    bjds = np.concatenate(bjds_list)\n",
    "    \n",
    "    assert len(ccf_data.shape) == 2\n",
    "    assert len(labels.shape) == 1\n",
    "    assert len(bjds.shape) == 1\n",
    "    assert ccf_data.shape[0] == labels.shape[0]\n",
    "    #print(\"Read dataset with {} examples\".format(labels.shape[0]))\n",
    "    return ccf_data, labels, bjds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ridge_regress_harps (No TF changes needed)\n",
    "def ridge_regress_harps(TRAIN_FILE_NAME_LIST, VAL_FILE_NAME_LIST, alpha, verbose, ccf_len):\n",
    "    rms_avg_list = []\n",
    "    weight_decay_list_t = []\n",
    "    gaussian_noise_list_t = []\n",
    "    rms_x_list = []\n",
    "\n",
    "    all_bjds_val = []\n",
    "    all_pred_val = []\n",
    "    all_labels_val = []\n",
    "    all_mean_val_preds = []\n",
    "    all_mean_val_bjds = []\n",
    "    all_mean_val_labels = []\n",
    "    all_mean_val_bjds = []\n",
    "    avg_list = []\n",
    "\n",
    "    for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "        TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "        VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "        train_X, train_Y, train_bjd  = load_dataset_ridge(TRAIN_FILE_NAME, ccf_len)\n",
    "        val_X, val_Y, val_bjd = load_dataset_ridge(VAL_FILE_NAME, ccf_len)\n",
    "\n",
    "        if train_X.size == 0 or val_X.size == 0:\n",
    "            print(f\"Skipping fold {index+1} due to missing data.\")\n",
    "            continue\n",
    "\n",
    "        pred_run_val = []\n",
    "        labels_run_val = []\n",
    "        bjd_run_val = []\n",
    "        for k in range(0,10):\n",
    "            model = Ridge(alpha=alpha).fit(train_X, train_Y)\n",
    "            val_pred_Y = model.predict(val_X)\n",
    "            pred_run_val.append(val_pred_Y)\n",
    "            labels_run_val.append(val_Y)\n",
    "            bjd_run_val.append(val_bjd)\n",
    "            rms_avg = np.sqrt(np.mean(np.square(val_Y -val_pred_Y)))\n",
    "            rms_avg_list.append(rms_avg)\n",
    "            if verbose == True:\n",
    "                print(model)\n",
    "                print(\"________________________\")\n",
    "                print(\"Cross-val number: \"+str(index+1)+\", Run number: \"+str(k+1))\n",
    "                print(\"rms: \"+str(rms_avg))\n",
    "            else:\n",
    "                continue\n",
    "        if pred_run_val: # Check if list is populated\n",
    "            mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "            mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "            mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "            all_mean_val_preds.append(mean_val_preds.tolist())\n",
    "            all_mean_val_labels.append(mean_val_labels.tolist())\n",
    "            all_mean_val_bjds.append(mean_val_bjds.tolist())\n",
    "    \n",
    "    if rms_avg_list:\n",
    "        avg = np.mean(rms_avg_list)\n",
    "        avg_list.append(avg)\n",
    "        print(\"________________________\")\n",
    "        print(\"average rms = \"+str(avg)+\" m/s\")\n",
    "    else:\n",
    "        print(\"No RMS values to average.\")\n",
    "        avg_list.append(np.nan)\n",
    "\n",
    "    #flatten the lists\n",
    "    all_mean_val_preds = [item for sublist in all_mean_val_preds for item in sublist]\n",
    "    all_mean_val_labels = [item for sublist in all_mean_val_labels for item in sublist]\n",
    "    all_mean_val_bjds = [item for sublist in all_mean_val_bjds for item in sublist]\n",
    "  \n",
    "    return all_mean_val_preds, all_mean_val_labels, all_mean_val_bjds, avg_list, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation Ridge Regression Model Run (10 times)\n",
    "\n",
    "all_mean_val_preds_ridge, all_mean_val_labels_ridge, all_mean_val_bjds_ridge, avg_rms_list_ridge, alpha_ridge = ridge_regress_harps(TRAIN_FILE_NAME_LIST, \n",
    "                                                                                 VAL_FILE_NAME_LIST, \n",
    "                                                                                 alpha=3.6094,#340.010636,#9.469, #340.010636,\t#335.734079,#9.469, \n",
    "                                                                                 verbose=False,\n",
    "                                                                                 ccf_len = ccf_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82543e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_mean_val_labels_ridge: # Only run if predictions were made\n",
    "    df = pd.DataFrame(list(zip(all_mean_val_labels_ridge, all_mean_val_preds_ridge, all_mean_val_bjds_ridge)), \n",
    "                   columns =['labels', 'preds', 'BJD']) \n",
    "    df_sorted = df.sort_values(by=['BJD'])\n",
    "    #df_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/cross_val_preds/val_preds_ridge_06_30_2021.xlsx', index = False)\n",
    "\n",
    "    # calculation of scatter removed\n",
    "    from astropy.stats import median_absolute_deviation\n",
    "\n",
    "    #df_sorted = df_from_excel\n",
    "    labels_68_percent_by_2 = (np.percentile(df_sorted[\"labels\"], 84)-np.percentile(df_sorted[\"labels\"], 16))/2\n",
    "    labels_median_absolute_deviation_norm = median_absolute_deviation(df_sorted[\"labels\"])*1.4826\n",
    "    preds_68_percent_by_2 = (np.percentile(df_sorted[\"preds\"], 84)-np.percentile(df_sorted[\"preds\"], 16))/2\n",
    "\n",
    "    corrected_rvs = df_sorted[\"labels\"]-df_sorted[\"preds\"]\n",
    "    corrected_rvs_68_percent_by_2 = (np.percentile(corrected_rvs, 84)-np.percentile(corrected_rvs, 16))/2\n",
    "    corrected_rvs_median_absolute_deviation_norm = median_absolute_deviation(corrected_rvs)*1.4826\n",
    "\n",
    "    sd_labels = np.std(df_sorted[\"labels\"], ddof=1)\n",
    "    sd_corrected_rv = np.std(corrected_rvs, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(df_sorted[\"labels\"] - df_sorted[\"preds\"])))\n",
    "\n",
    "\n",
    "    print(\"labels: sd: \"+str(sd_labels))\n",
    "    print(\"corrected rv: sd: \"+str(sd_corrected_rv))\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"labels: MAD*1.4826: \"+str(labels_median_absolute_deviation_norm))\n",
    "    print(\"corrected rv: MAD*1.4826: \"+str(corrected_rvs_median_absolute_deviation_norm))\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"labels: 68 percent/2: \"+str(labels_68_percent_by_2))\n",
    "    print(\"corrected rv: 68 percent/2: \"+str(corrected_rvs_68_percent_by_2))\n",
    "\n",
    "\n",
    "    # plot the average results\n",
    "    # Scatter reduction plot\n",
    "    rms_avg_list = []\n",
    "    sd_x = np.std(all_mean_val_labels_ridge, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(all_mean_val_labels_ridge - np.array(all_mean_val_preds_ridge))))\n",
    "    #rms_x_list.append(rms_x)\n",
    "    rms_avg_list.append(rms_x)\n",
    "    stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "    x_range = np.linspace(-4,5.5, 17)\n",
    "    upper_bound = x_range+rms_x\n",
    "    lower_bound = x_range-rms_x\n",
    "\n",
    "    # plot the scatter reduction plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(21, 6))\n",
    "    ax1 = ax[0]\n",
    "    ax1.plot(df_sorted[\"labels\"], df_sorted[\"preds\"], \".\")\n",
    "    ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "    #ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "    rms_fill=rms_x#0.15\n",
    "    ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                    alpha=0.5, label=\"1 standard deviation\")\n",
    "    ax1.set_xlim(-4, 5);\n",
    "    ax1.set_ylim(-4, 5);\n",
    "    ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "    ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "    ax1.set_title(\"Ridge Regression Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "    #hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "    textstr = '\\n'.join((\n",
    "        r'Raw scatter=%.3f m/s' % (sd_labels, ),\n",
    "        r'Corrected scatter=%.3f m/s' % (sd_corrected_rv, ),\n",
    "        r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "    ax1.text(-3.8, 4.5, textstr, size=15,\n",
    "            ha=\"left\", va=\"top\",\n",
    "            bbox=dict(facecolor='#fdcf44',edgecolor='k',\n",
    "                      boxstyle=\"square\",\n",
    "                      #ec=(1., 0.5, 0.5),\n",
    "                      #fc=(1., 0.8, 0.8),\n",
    "                      ))\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot predictions over time\n",
    "    ax2 = ax[1]\n",
    "    ax2.plot(df_sorted[\"BJD\"], df_sorted[\"labels\"], \".\",color='k',\n",
    "             markersize=10,label=\"labels\")\n",
    "    ax2.plot(df_sorted[\"BJD\"], df_sorted[\"preds\"], \".\", markersize=10,\n",
    "             label=\"preds\")\n",
    "    ax2.set_xlabel(\"Time (BJD)\")\n",
    "    ax2.set_ylabel(\"RV (m/s)\")\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "else:\n",
    "    print(\"Skipping Ridge Regression plots because no predictions were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_mean_val_labels_ridge: # Only run if previous cell worked\n",
    "    index=0\n",
    "    TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "    VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "    train_X, train_Y, train_bjd  = load_dataset_ridge(TRAIN_FILE_NAME, ccf_len)\n",
    "    val_X, val_Y, val_bjd = load_dataset_ridge(VAL_FILE_NAME, ccf_len)\n",
    "    model = Ridge(alpha=alpha_ridge).fit(train_X, train_Y)\n",
    "    plt.plot(model.coef_, \".\")\n",
    "    plt.title(\"Ridge Model Coefficients\")\n",
    "    plt.xlabel(\"Coefficient Index\")\n",
    "    plt.ylabel(\"Coefficient Value\")\n",
    "    plt.show()\n",
    "    \n",
    "    residual_plot(train_Y, np.linspace(-19,19,ccf_len), train_X, \"median\",\"Old Test set: Residual CCFs\")\n",
    "else:\n",
    "    print(\"Skipping Ridge coefficient plot, no model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'val_Y' in locals(): # Check if val_Y was defined in the previous cell\n",
    "    # Plot old dataset\n",
    "    residual_plot(val_Y, np.linspace(-20,20,ccf_len), val_X, \"median\",\"Old Test set: Residual CCFs\")\n",
    "\n",
    "    df_old = pd.DataFrame(list(zip(val_Y,val_X, val_bjd)), columns =['val_Y', 'val_X', 'BJD']) \n",
    "    df_old_sorted = df_old.sort_values(by=['BJD'])\n",
    "    df_old_sorted = df_old_sorted.reset_index(drop=True)\n",
    "else:\n",
    "    print(\"Skipping residual plot, val_Y not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25971ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha cross-val runs\n",
    "\n",
    "# Cross validation Ridge Regression Model Run (10 times)\n",
    "alpha_range = np.random.uniform(2, 5.5, 100)\n",
    "#alpha_range = np.append(alpha_range, 9.469)\n",
    "\n",
    "alpha_list = []\n",
    "rms_crossval_list = []\n",
    "for alpha in tqdm(alpha_range):\n",
    "    all_mean_val_preds, all_mean_val_labels, all_mean_val_bjds, avg_rms_list, alpha_out = ridge_regress_harps(TRAIN_FILE_NAME_LIST, \n",
    "                                                                                  VAL_FILE_NAME_LIST, \n",
    "                                                                                  alpha=alpha, \n",
    "                                                                                  verbose=False,\n",
    "                                                                                  ccf_len= ccf_len)\n",
    "    # compute rms across all crossval slice\n",
    "    rms_crossval = np.mean(avg_rms_list)\n",
    "    rms_crossval_list.append(rms_crossval)\n",
    "    alpha_list.append(alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c642595",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rms = pd.DataFrame(list(zip(alpha_list, rms_crossval_list)), \n",
    "               columns =['alpha_list', 'avg_rms_list'])\n",
    "df_rms_sorted = df_rms.sort_values(by=['avg_rms_list'])\n",
    "#df_rms_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/Clean_June28_rv_net_code/crossval_preds/alpha_rms_linear_12_6.xlsx', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell tries to read from Google Drive. It will fail unless GDrive is mounted.\n",
    "# You can mount your drive with the following:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "try:\n",
    "    # read in the files\n",
    "    df_rms_sorted_1 = pd.read_excel('gdrive/Shared drives/Exoplanet_RV/Clean_June28_rv_net_code/crossval_preds/alpha_rms_linear_12_1.xlsx')\n",
    "    df_rms_sorted_2 = pd.read_excel('gdrive/Shared drives/Exoplanet_RV/Clean_June28_rv_net_code/crossval_preds/alpha_rms_linear_12_1_v2.xlsx')\n",
    "\n",
    "    # Combine dataframes\n",
    "    frames = [df_rms_sorted_1, df_rms_sorted_2]\n",
    "    df_rms_sorted_gdrive = pd.concat(frames)\n",
    "\n",
    "    df_rms_sorted_gdrive = df_rms_sorted_gdrive.sort_values(by=['avg_rms_list'])\n",
    "    print(df_rms_sorted_gdrive.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find Google Drive files. Using results from this run.\")\n",
    "    print(df_rms_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "plt.plot(df_rms_sorted[\"alpha_list\"],df_rms_sorted[\"avg_rms_list\"], \".\", color='purple',\n",
    "         markersize=9)\n",
    "plt.title(\"Linear Model Performance as a Function of Alpha\")\n",
    "plt.xlabel(\"Alpha Value\")\n",
    "plt.ylabel(\"RMS (m/s)\")\n",
    "#plt.xlim(0, 20)\n",
    "#plt.ylim(0.94, 0.95)\n",
    "\n",
    "if not df_rms_sorted.empty:\n",
    "    print(\"Best alpha from this run:\")\n",
    "    print(df_rms_sorted.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc290d5",
   "metadata": {},
   "source": [
    "## FC NN Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title FC NN Model run (Updated)\n",
    "\n",
    "print(\"Starting FC NN Model Run...\")\n",
    "\n",
    "# Cross validation Linear NN Model Run (10 times)\n",
    "rms_avg_list = []\n",
    "weight_decay_list_t = []\n",
    "gaussian_noise_list_t = []\n",
    "rms_x_list = []\n",
    "\n",
    "all_bjds_val = []\n",
    "all_pred_val = []\n",
    "all_labels_val = []\n",
    "all_mean_val_preds = []\n",
    "all_mean_val_bjds = []\n",
    "all_mean_val_labels = []\n",
    "\n",
    "for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "    TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "    VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "    bjd_run_val = []\n",
    "    pred_run_val = []\n",
    "    labels_run_val = []\n",
    "    all_bjds_val = [] # Re-init for this fold\n",
    "    \n",
    "    for k in range(0,1):#0): #should be 10 for a full run\n",
    "        hparams = SimpleNamespace(**dict(\n",
    "          num_features=ccf_len, # Use ccf_len variable\n",
    "          learning_rate=0.0054042, #0.040267, #0.0095352,#0.0016077,\n",
    "          momentum=0.9, # Not used by AdamW\n",
    "          batch_size=300,\n",
    "          num_dense_units=100,#1000,#200,\n",
    "          num_dense_layers=8, #4,\n",
    "          weight_decay=0.00010000,#0.008000,#0.00010000, #5e-4, #7e-2,\n",
    "          gaussian_noise_scale=0,#1.5, \n",
    "        ))\n",
    "        model = FCModel(hparams)\n",
    "        train(model, hparams, plots=\"ON\",model_name=\"FC NN\", num_epochs=50)\n",
    "        print(model)\n",
    "        print(\"________________________\")\n",
    "        print(\"Cross-val number: \"+str(index+1)+\", Run number: \"+str(k+1))\n",
    "        \n",
    "    if pred_run_val: # Only calculate mean if list is not empty\n",
    "        mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "        mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "        mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "        all_mean_val_preds.append(mean_val_preds.tolist())\n",
    "        all_mean_val_labels.append(mean_val_labels.tolist())\n",
    "        all_mean_val_bjds.append(mean_val_bjds.tolist())\n",
    "    else:\n",
    "        print(f\"Warning: No predictions recorded for fold {index+1}\")\n",
    "\n",
    "avg = np.mean(rms_avg_list)\n",
    "print(\"________________________\")\n",
    "print(f\"Final Average RMS: {avg}\")\n",
    "\n",
    "#flatten the lists\n",
    "all_mean_val_preds_flat = [item for sublist in all_mean_val_preds for item in sublist]\n",
    "all_mean_val_labels_flat = [item for sublist in all_mean_val_labels for item in sublist]\n",
    "all_mean_val_bjds_flat = [item for sublist in all_mean_val_bjds for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961426f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_mean_val_labels_flat: # Only run if predictions were made\n",
    "    df = pd.DataFrame(list(zip(all_mean_val_labels_flat,all_mean_val_preds_flat, all_mean_val_bjds_flat)), \n",
    "                   columns =['labels', 'preds', 'BJD']) \n",
    "    df_sorted = df.sort_values(by=['BJD'])\n",
    "    #df_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/cross_val_preds/val_preds_FCNN_06_30_2021.xlsx', index = False)\n",
    "\n",
    "    # calculation of scatter removed\n",
    "    from astropy.stats import median_absolute_deviation\n",
    "\n",
    "    #df_sorted = df_from_excel\n",
    "    labels_68_percent_by_2 = (np.percentile(df_sorted[\"labels\"], 84)-np.percentile(df_sorted[\"labels\"], 16))/2\n",
    "    labels_median_absolute_deviation_norm = median_absolute_deviation(df_sorted[\"labels\"])*1.4826\n",
    "    preds_68_percent_by_2 = (np.percentile(df_sorted[\"preds\"], 84)-np.percentile(df_sorted[\"preds\"], 16))/2\n",
    "\n",
    "    corrected_rvs = df_sorted[\"labels\"]-df_sorted[\"preds\"]\n",
    "    corrected_rvs_68_percent_by_2 = (np.percentile(corrected_rvs, 84)-np.percentile(corrected_rvs, 16))/2\n",
    "    corrected_rvs_median_absolute_deviation_norm = median_absolute_deviation(corrected_rvs)*1.4826\n",
    "\n",
    "    sd_labels = np.std(df_sorted[\"labels\"], ddof=1)\n",
    "    sd_corrected_rv = np.std(corrected_rvs, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(df_sorted[\"labels\"] - df_sorted[\"preds\"])))\n",
    "\n",
    "\n",
    "    print(\"labels: sd: \"+str(sd_labels))\n",
    "    print(\"corrected rv: sd: \"+str(sd_corrected_rv))\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"labels: MAD*1.4826: \"+str(labels_median_absolute_deviation_norm))\n",
    "    print(\"corrected rv: MAD*1.4826: \"+str(corrected_rvs_median_absolute_deviation_norm))\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"labels: 68 percent/2: \"+str(labels_68_percent_by_2))\n",
    "    print(\"corrected rv: 68 percent/2: \"+str(corrected_rvs_68_percent_by_2))\n",
    "\n",
    "\n",
    "    # plot the average results\n",
    "    # Scatter reduction plot\n",
    "    rms_avg_list = []\n",
    "    sd_x = np.std(all_mean_val_labels_flat, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(all_mean_val_labels_flat - np.array(all_mean_val_preds_flat))))\n",
    "    #rms_x_list.append(rms_x)\n",
    "    rms_avg_list.append(rms_x)\n",
    "    stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "    x_range = np.linspace(-4,5.5, 17)\n",
    "    upper_bound = x_range+rms_x\n",
    "    lower_bound = x_range-rms_x\n",
    "\n",
    "    # plot the scatter reduction plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(21, 6))\n",
    "    ax1 = ax[0]\n",
    "    ax1.plot(df_sorted[\"labels\"], df_sorted[\"preds\"], \".\")\n",
    "    ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "    #ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "    rms_fill=rms_x#0.15\n",
    "    ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                    alpha=0.5, label=\"1 standard deviation\")\n",
    "    ax1.set_xlim(-4, 5);\n",
    "    ax1.set_ylim(-4, 5);\n",
    "    ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "    ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "    ax1.set_title(\"FC NN Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "    #hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "    textstr = '\\n'.join((\n",
    "        r'Raw scatter=%.3f m/s' % (labels_68_percent_by_2, ),\n",
    "        r'Corrected scatter=%.3f m/s' % (corrected_rvs_68_percent_by_2, ),\n",
    "        r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "    ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "            ha=\"left\", va=\"top\",\n",
    "            bbox=dict(facecolor='#fdcf44',edgecolor='k',\n",
    "                      boxstyle=\"square\",\n",
    "                      #ec=(1., 0.5, 0.5),\n",
    "                      #fc=(1., 0.8, 0.8),\n",
    "                      ))\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot predictions over time\n",
    "    ax2 = ax[1]\n",
    "    ax2.plot(df_sorted[\"BJD\"], df_sorted[\"labels\"], \".\",color='k',\n",
    "             markersize=10,label=\"labels\")\n",
    "    ax2.plot(df_sorted[\"BJD\"], df_sorted[\"preds\"], \".\", markersize=10,\n",
    "             label=\"preds\")\n",
    "    ax2.set_xlabel(\"Time (BJD)\")\n",
    "    ax2.set_ylabel(\"RV (m/s)\")\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "else:\n",
    "    print(\"Skipping FC NN plots because no predictions were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc8335",
   "metadata": {},
   "source": [
    "## CNN Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title CNN Model run (Updated)\n",
    "\n",
    "print(\"Starting CNN Model Run...\")\n",
    "\n",
    "# Cross validation CNN Model Run (10 times)\n",
    "rms_avg_list = []\n",
    "weight_decay_list_t = []\n",
    "gaussian_noise_list_t = []\n",
    "rms_x_list = []\n",
    "\n",
    "all_bjds_val = []\n",
    "all_pred_val = []\n",
    "all_labels_val = []\n",
    "all_mean_val_preds = []\n",
    "all_mean_val_bjds = []\n",
    "all_mean_val_labels = []\n",
    "\n",
    "for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "    TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "    VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "    bjd_run_val = []\n",
    "    pred_run_val = []\n",
    "    labels_run_val = []\n",
    "    all_bjds_val = [] # Re-init for this fold\n",
    "    \n",
    "    for k in range(0,2):#0): #should be 10 for a full run\n",
    "        hparams = SimpleNamespace(**dict(\n",
    "            num_features=ccf_len, # Use ccf_len variable\n",
    "            learning_rate=0.0085099,#0.0050618, #1e-3,\n",
    "            momentum=0.9, # Not used by AdamW\n",
    "            batch_size=300,\n",
    "            conv_kernel_size=3,\n",
    "            num_conv_filters=16, #32,\n",
    "            num_conv_layers=2,#4,\n",
    "            num_dense_units=100,#500,\n",
    "            num_dense_layers=1,\n",
    "            weight_decay=0.0017600,#0.0012341, #5e-4, #7e-2,\n",
    "            gaussian_noise_scale=0,#1.5,\n",
    "        ))\n",
    "        model = CNNModel(hparams)\n",
    "        train(model, hparams, plots=\"OFF\",model_name=\"CNN\", num_epochs=90)#35)#65)\n",
    "        print(model)\n",
    "        print(\"________________________\")\n",
    "        print(\"Cross-val number: \"+str(index+1)+\", Run number: \"+str(k+1))\n",
    "        \n",
    "    if pred_run_val: # Only calculate mean if list is not empty\n",
    "        mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "        mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "        mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "        all_mean_val_preds.append(mean_val_preds.tolist())\n",
    "        all_mean_val_labels.append(mean_val_labels.tolist())\n",
    "        all_mean_val_bjds.append(mean_val_bjds.tolist())\n",
    "    else:\n",
    "        print(f\"Warning: No predictions recorded for fold {index+1}\")\n",
    "\n",
    "avg = np.mean(rms_avg_list)\n",
    "print(\"________________________\")\n",
    "print(f\"Final Average RMS: {avg}\")\n",
    "\n",
    "#flatten the lists\n",
    "all_mean_val_preds_flat = [item for sublist in all_mean_val_preds for item in sublist]\n",
    "all_mean_val_labels_flat = [item for sublist in all_mean_val_labels for item in sublist]\n",
    "all_mean_val_bjds_flat = [item for sublist in all_mean_val_bjds for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_mean_val_labels_flat: # Only run if predictions were made\n",
    "    df = pd.DataFrame(list(zip(all_mean_val_labels_flat, all_mean_val_preds_flat, all_mean_val_bjds_flat)), \n",
    "                   columns =['labels', 'preds', 'BJD']) \n",
    "    df_sorted = df.sort_values(by=['BJD'])\n",
    "    #df_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/cross_val_preds/val_preds_CNN_06_30_2021.xlsx', index = False)\n",
    "\n",
    "    # calculation of scatter removed\n",
    "    from astropy.stats import median_absolute_deviation\n",
    "\n",
    "    #df_sorted = df_from_excel\n",
    "    labels_68_percent_by_2 = (np.percentile(df_sorted[\"labels\"], 84)-np.percentile(df_sorted[\"labels\"], 16))/2\n",
    "    labels_median_absolute_deviation_norm = median_absolute_deviation(df_sorted[\"labels\"])*1.4826\n",
    "    preds_68_percent_by_2 = (np.percentile(df_sorted[\"preds\"], 84)-np.percentile(df_sorted[\"preds\"], 16))/2\n",
    "\n",
    "    corrected_rvs = df_sorted[\"labels\"]-df_sorted[\"preds\"]\n",
    "    corrected_rvs_68_percent_by_2 = (np.percentile(corrected_rvs, 84)-np.percentile(corrected_rvs, 16))/2\n",
    "    corrected_rvs_median_absolute_deviation_norm = median_absolute_deviation(corrected_rvs)*1.4826\n",
    "\n",
    "    sd_labels = np.std(df_sorted[\"labels\"], ddof=1)\n",
    "    sd_corrected_rv = np.std(corrected_rvs, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(df_sorted[\"labels\"] - df_sorted[\"preds\"])))\n",
    "\n",
    "\n",
    "    print(\"labels: sd: \"+str(sd_labels))\n",
    "    print(\"corrected rv: sd: \"+str(sd_corrected_rv))\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"labels: MAD*1.4826: \"+str(labels_median_absolute_deviation_norm))\n",
    "    print(\"corrected rv: MAD*1.4826: \"+str(corrected_rvs_median_absolute_deviation_norm))\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"labels: 68 percent/2: \"+str(labels_68_percent_by_2))\n",
    "    print(\"corrected rv: 68 percent/2: \"+str(corrected_rvs_68_percent_by_2))\n",
    "\n",
    "\n",
    "    # plot the average results\n",
    "    # Scatter reduction plot\n",
    "    rms_avg_list = []\n",
    "    sd_x = np.std(all_mean_val_labels_flat, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(all_mean_val_labels_flat - np.array(all_mean_val_preds_flat))))\n",
    "    #rms_x_list.append(rms_x)\n",
    "    rms_avg_list.append(rms_x)\n",
    "    stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "    x_range = np.linspace(-4,5.5, 17)\n",
    "    upper_bound = x_range+rms_x\n",
    "    lower_bound = x_range-rms_x\n",
    "\n",
    "    # plot the scatter reduction plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(21, 6))\n",
    "    ax1 = ax[0]\n",
    "    ax1.plot(df_sorted[\"labels\"], df_sorted[\"preds\"], \".\")\n",
    "    ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "    #ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "    rms_fill=rms_x#0.15\n",
    "    ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                    alpha=0.5, label=\"1 standard deviation\")\n",
    "    ax1.set_xlim(-4, 5);\n",
    "    ax1.set_ylim(-4, 5);\n",
    "    ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "    ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "    ax1.set_title(\"CNN Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "    #hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "    textstr = '\\n'.join((\n",
    "        r'Raw scatter=%.3f m/s' % (labels_68_percent_by_2, ),\n",
    "        r'Corrected scatter=%.3f m/s' % (corrected_rvs_68_percent_by_2, ),\n",
    "        r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "    ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "            ha=\"left\", va=\"top\",\n",
    "            bbox=dict(facecolor='#fdcf44',edgecolor='k',\n",
    "                      boxstyle=\"square\",\n",
    "                      #ec=(1., 0.5, 0.5),\n",
    "                      #fc=(1., 0.8, 0.8),\n",
    "                      ))\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot predictions over time\n",
    "    ax2 = ax[1]\n",
    "    ax2.plot(df_sorted[\"BJD\"], df_sorted[\"labels\"], \".\",color='k',\n",
    "             markersize=10,label=\"labels\")\n",
    "    ax2.plot(df_sorted[\"BJD\"], df_sorted[\"preds\"], \".\", markersize=10,\n",
    "             label=\"preds\")\n",
    "    ax2.set_xlabel(\"Time (BJD)\")\n",
    "    ax2.set_ylabel(\"RV (m/s)\")\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "else:\n",
    "    print(\"Skipping CNN plots because no predictions were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a1d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
