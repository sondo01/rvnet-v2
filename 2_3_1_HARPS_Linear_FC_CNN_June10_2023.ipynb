{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce028cf1",
   "metadata": {},
   "source": [
    "## HARPS-N Solar Data Neural Networks Code\n",
    "\n",
    "written by Zoe L. de Beurs \n",
    "\n",
    "See [de Beurs, Zoe L., Vanderburg, A., Shallue, C.J., et al. (2022)](https://iopscience.iop.org/article/10.3847/1538-3881/ac738e/pdf) for more details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19df0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "#Ensures we use Tensorflow 1.x\n",
    "#%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "# import tensorflow_addons as tfa\n",
    "print(tf.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 15\n",
    "\n",
    "#from google.colab import files\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from astropy.stats import median_absolute_deviation\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d207c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/zdebeurs/rv_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea11de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rv_net imports.\n",
    "import sys\n",
    "sys.path.append(\"rv_net/\")\n",
    "from ops import training\n",
    "from tf_util import config_util\n",
    "from tf_util import configdict\n",
    "from tf_util import estimator_runner\n",
    "from rv_net import data_HARPS_N\n",
    "from rv_net import  data, rv_model, estimator_util, load_dataset_ridge, ridge_regress_harps\n",
    "#from ridge_regress_harps import ridge_regress_harps\n",
    "#from load_dataset_ridge import load_dataset_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Residual plotting code\n",
    "import matplotlib as mpl\n",
    "def residual_plot(rv_list, x_axis, ccfs_of_interest, num_ref_ccf, plot_title):\n",
    "    # create color scheme\n",
    "    min_rv = np.min(rv_list)\n",
    "    max_rv = np.max(rv_list)\n",
    "    cscale_residuals = (np.array(rv_list - min_rv) / (max_rv - min_rv))\n",
    "    print(np.min(cscale_residuals), np.max(cscale_residuals))\n",
    "\n",
    "    col = plt.cm.jet([0.25, 0.75])\n",
    "    n = len(ccfs_of_interest)\n",
    "    colors = plt.cm.bwr(cscale_residuals)\n",
    "\n",
    "    # Create the residual plot by looping through the list of CCFs ordered by date\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "    num = 0\n",
    "    for i in np.arange(0, len(ccfs_of_interest)):\n",
    "        if num_ref_ccf == \"median\":\n",
    "            plt.plot(x_axis, ccfs_of_interest[i] - np.median(list(ccfs_of_interest), axis=0), color=colors[num])\n",
    "        else:\n",
    "            if i != num_ref_ccf:\n",
    "                plt.plot(x_axis, ccfs_of_interest[i] - ccfs_of_interest[num_ref_ccf], color=colors[num])\n",
    "        num += 1\n",
    "\n",
    "    plt.title(plot_title)\n",
    "    # make color bar\n",
    "    cmap = mpl.cm.bwr\n",
    "    norm = mpl.colors.Normalize(vmin=(min_rv - np.median(rv_list)), vmax=(max_rv - np.median(rv_list)))\n",
    "    cb = plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), orientation=\"vertical\", pad=-0.0001)\n",
    "    cb.set_label(label='Stellar Activity Signal (m/s)', size=16, rotation=270, labelpad=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef1f9e6",
   "metadata": {},
   "source": [
    "## Reading in the data (June 29, 2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74efcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_method ='cross_val' #\"val\"# \"val\"#\"cross_val\" # \"val\" # \"test\"\n",
    "\n",
    "ccf_len = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to point towards the location of the files on your device\n",
    "\n",
    "!ls 'Archive_HARPS_N_NEW DRS/TF_records_Nov2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files for cross-validation\n",
    "DATA_DIR = 'Archive_HARPS_N_NEW DRS/TF_records_Nov2021/' #TF_record_July_10_21_no_planets_median_prov_rvs/' #HARPS-N Solar Telescope Data (using old DRS)/' #TF_record_July_10_21_no_planets_same_test_set' #TF_record_July_10_21_no_planets_v2'\n",
    "\n",
    "if eval_method ==\"cross_val\":\n",
    "    data_files = tf.data.Dataset.list_files(DATA_DIR+'*cross_val*',shuffle=False)\n",
    "    data_files = [t.numpy() for t in data_files]\n",
    "    TRAIN_FILE_NAME_LIST = []\n",
    "    VAL_FILE_NAME_LIST = []\n",
    "\n",
    "    N = len(data_files)\n",
    "    for i in range(N):\n",
    "        val_files = [data_files[i]]\n",
    "        #print(val_files)\n",
    "        VAL_FILE_NAME_LIST.append(val_files)\n",
    "        train_files = data_files[0:i] + data_files[i+1:]\n",
    "        TRAIN_FILE_NAME_LIST.append(train_files)\n",
    "        # add all the training files\n",
    "\n",
    "    NUM_TRAINING_EXAMPLES = 503\n",
    "    NUM_VALIDATION_EXAMPLES = 51\n",
    "\n",
    "elif eval_method ==\"val\":\n",
    "    TRAIN_FILE_NAME_LIST = [[os.path.join(DATA_DIR, \"TF_ccf_full_train\")]]\n",
    "    VAL_FILE_NAME_LIST = [[os.path.join(DATA_DIR, \"TF_ccf_val\")]]#test\")]]\n",
    "\n",
    "    NUM_TRAINING_EXAMPLES = 503\n",
    "    NUM_VALIDATION_EXAMPLES = 61\n",
    "elif eval_method ==\"test\":\n",
    "    TRAIN_FILE_NAME_LIST = [[os.path.join(DATA_DIR, \"TF_ccf_full_train\")]]\n",
    "    VAL_FILE_NAME_LIST = [[os.path.join(DATA_DIR, \"TF_ccf_test\")]]#test\")]]\n",
    "\n",
    "    NUM_TRAINING_EXAMPLES = 503\n",
    "    NUM_VALIDATION_EXAMPLES = 61\n",
    "else:\n",
    "    print(\"Please select a valid evaluation method: 'cross_val' or 'val' or 'test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362890ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRAIN_FILE_NAME_LIST)\n",
    "VAL_FILE_NAME_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ad87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data loader\n",
    "def _example_parser(serialized_example):\n",
    "    \"\"\"Parses a single tf.Example into feature and label tensors.\"\"\"\n",
    "    feature_name = \"Rescaled CCF_residuals_cutoff\"#\"Rescaled CCF_residuals\" #CCF_residuals\n",
    "    label_name = \"activity signal\"#\"RV\",\n",
    "    label2_name = \"BJD\"\n",
    "    data_fields = {\n",
    "        feature_name: tf.io.FixedLenFeature([ccf_len], tf.float32), #[161], tf.float32),\n",
    "        label_name: tf.io.FixedLenFeature([], tf.float32),\n",
    "        label2_name: tf.io.FixedLenFeature([], tf.float32),\n",
    "    }\n",
    "    parsed_fields = tf.io.parse_single_example(serialized_example, features=data_fields)\n",
    "    return parsed_fields[feature_name], parsed_fields[label_name]*1000, parsed_fields[label2_name]\n",
    "\n",
    "\n",
    "def load_dataset(filenames, batch_size, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    dataset = filename_dataset.flat_map(tf.data.TFRecordDataset)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=NUM_TRAINING_EXAMPLES)\n",
    "    dataset = dataset.map(_example_parser, num_parallel_calls=4)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define LinearModel, FCModel, CNNModel\n",
    "class LinearModel(tf.keras.Model):\n",
    "    \"\"\"A TensorFlow linear regression model.\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"Basic setup.\n",
    "\n",
    "        Args:\n",
    "          hparams: A ConfigDict of hyperparameters for building the model.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: If mode is invalid.\n",
    "        \"\"\"\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        #self.weights = tf.Variable(tf.zeros(self.hparams.num_features))\n",
    "        self.dense_layer = tf.keras.layers.Dense(\n",
    "            1, kernel_initializer=tf.zeros_initializer, use_bias=False)\n",
    "        \n",
    "    def call(self, features, training=False):\n",
    "      # return tf.tensordot(features, self.weights, axes=1)\n",
    "      return tf.squeeze(self.dense_layer(features))\n",
    "\n",
    "class FCModel(tf.keras.Model):\n",
    "    \"\"\"A TensorFlow linear regression model.\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"Basic setup.\n",
    "\n",
    "        Args:\n",
    "          hparams: A ConfigDict of hyperparameters for building the model.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: If mode is invalid.\n",
    "        \"\"\"\n",
    "        super(FCModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        #self.hidden_layer1 = tf.keras.layers.Dense(\n",
    "        #    self.hparams.num_dense_units, activation=tf.keras.activations.relu)\n",
    "        self.dense_layers = [\n",
    "          tf.keras.layers.Dense(\n",
    "              hparams.num_dense_units,\n",
    "              activation=tf.keras.activations.relu)\n",
    "          for i in range(hparams.num_dense_layers)\n",
    "        ]\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, training=False):\n",
    "        net = tf.expand_dims(features, -1)\n",
    "        batch_size, length, depth = net.shape\n",
    "        net = tf.reshape(net, [batch_size, length*depth])\n",
    "        for dense in self.dense_layers:\n",
    "            net = dense(net)\n",
    "        net = self.output_layer(net)\n",
    "        return tf.squeeze(net)\n",
    "\n",
    "# @title Define RVLinearModel\n",
    "class CNNModel(tf.keras.Model):\n",
    "    \"\"\"A TensorFlow linear regression model.\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"Basic setup.\n",
    "\n",
    "        Args:\n",
    "          hparams: A ConfigDict of hyperparameters for building the model.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: If mode is invalid.\n",
    "        \"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.conv_layers = [\n",
    "          tf.keras.layers.Conv1D(\n",
    "              filters=hparams.num_conv_filters, \n",
    "              kernel_size=hparams.conv_kernel_size,\n",
    "              activation=tf.keras.activations.relu,\n",
    "              padding=\"same\")\n",
    "          for i in range(hparams.num_conv_layers)\n",
    "        ]\n",
    "        self.dense_layers = [\n",
    "          tf.keras.layers.Dense(\n",
    "              hparams.num_dense_units,\n",
    "              activation=tf.keras.activations.relu)\n",
    "          for i in range(hparams.num_dense_layers)\n",
    "        ]\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, training=False):\n",
    "        net = tf.expand_dims(features, -1)\n",
    "        for conv in self.conv_layers:\n",
    "            net = conv(net)\n",
    "        batch_size, length, depth = net.shape\n",
    "        net = tf.reshape(net, [batch_size, length*depth])\n",
    "        for dense in self.dense_layers:\n",
    "            net = dense(net)\n",
    "        net = self.output_layer(net)\n",
    "        return tf.squeeze(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, dataset):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_bjds = []\n",
    "    for features, labels, bjds in dataset:\n",
    "        preds = model(features, training=False)\n",
    "        all_preds.append(preds.numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        all_bjds.append(bjds.numpy())\n",
    "    return np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_bjds)\n",
    "\n",
    "def train(model, hparams, plots, model_name, num_epochs):#=100):\n",
    "    train_dataset = load_dataset([TRAIN_FILE_NAME], batch_size=hparams.batch_size, mode=tf.estimator.ModeKeys.TRAIN)\n",
    "    val_dataset = load_dataset([VAL_FILE_NAME], batch_size=min(1024, NUM_VALIDATION_EXAMPLES), mode=tf.estimator.ModeKeys.EVAL)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    Opt = tfa.optimizers.extend_with_decoupled_weight_decay(tf.optimizers.SGD)\n",
    "    optimizer = Opt(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate, momentum=hparams.momentum)\n",
    "    metrics = [\n",
    "             tf.keras.metrics.MeanSquaredError(\"train_loss\"),\n",
    "             tf.keras.metrics.RootMeanSquaredError(\"train_rmse\")\n",
    "    ]\n",
    "    weight_decay_list_t.append(hparams.weight_decay)\n",
    "    gaussian_noise_list_t.append(hparams.gaussian_noise_scale)\n",
    "\n",
    "    metric_values = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Reset metric values for each new epoch.\n",
    "        for m in metrics:\n",
    "            m.reset_states()\n",
    "\n",
    "        # Train over all batches in the training set.\n",
    "        for features, labels, bjds in train_dataset:\n",
    "            if hparams.gaussian_noise_scale:\n",
    "                features += tf.random.normal(features.shape, stddev=hparams.gaussian_noise_scale)\n",
    "                #print(hparams.gaussian_noise_scale)\n",
    "            # One training step.\n",
    "            with tf.GradientTape() as t:\n",
    "                preds = model(features, training=True)\n",
    "                loss = loss_fn(labels, preds)\n",
    "            grads = t.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            # Update the metrics.\n",
    "            for m in metrics:\n",
    "                m(labels, preds)\n",
    "    \n",
    "        # End of an epoch.\n",
    "        epoch_metrics = {\"epoch\": epoch}\n",
    "        # First, log the training metrics.\n",
    "        for m in metrics:\n",
    "            epoch_metrics[m.name] = m.result().numpy()\n",
    "        # Next, evaluate over the validation set.\n",
    "        labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "        epoch_metrics[\"val_rmse\"] = np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "        # Add a metric for the raw scatter started with\n",
    "        epoch_metrics[\"original_rmse\"] = np.std(labels_val)\n",
    "        # Add a metric for raw scatter - corrected scatter\n",
    "        epoch_metrics[\"difference_rmse\"] = np.std(labels_val) - np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "        # Log metrics to tensorboard.\n",
    "        for metric, value in epoch_metrics.items():\n",
    "            tf.summary.scalar(metric, value, step=epoch)\n",
    "        epoch_metrics[\"epoch\"] = epoch\n",
    "        # Print metric values at selected epochs.\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            print(\"{epoch}: Train loss: {train_loss:.4}, Train RMSE: {train_rmse:.4}, Val RMSE: {val_rmse:.4}\".format(**epoch_metrics))\n",
    "        metric_values.append(epoch_metrics)\n",
    "\n",
    "    # Gather predictions\n",
    "    labels, preds, bjd = make_predictions(model, train_dataset)\n",
    "    labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "    all_bjds_val.append(bjd_val)\n",
    "    bjd_run_val.append(bjd_val)\n",
    "    all_pred_val.append(preds_val)\n",
    "    pred_run_val.append(preds_val)\n",
    "    all_labels_val.append(labels_val)\n",
    "    labels_run_val.append(labels_val)\n",
    "\n",
    "    # Scatter reduction plot\n",
    "    sd_x = np.std(labels_val, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(labels_val - preds_val)))\n",
    "    rms_x_list.append(rms_x)\n",
    "    rms_avg_list.append(rms_x)\n",
    "    stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "    x_range = np.linspace(-4,5.5, 17)\n",
    "    upper_bound = x_range+rms_x\n",
    "    lower_bound = x_range-rms_x\n",
    "\n",
    "    if plots==\"ON\":\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(24, 6))\n",
    "        ax = axes[0]\n",
    "        ax.plot([m[\"train_rmse\"] for m in metric_values], label=\"Train RMSE\")\n",
    "        ax.plot([m[\"val_rmse\"] for m in metric_values], label=\"Validation RMSE\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "        # Gather predictions to plot against labels.\n",
    "        ax = axes[1]\n",
    "        ax.plot(preds, labels, \".\", label=\"Training\")\n",
    "        ax.plot(preds_val, labels_val, \".\", label=\"Validation\")\n",
    "        ax.set_xlabel(\"Actual Y\")\n",
    "        ax.set_ylabel(\"Predicted Y\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # plot the scatter reduction plot\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        ax1.plot(labels_val, preds_val, \".\")\n",
    "        ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "        #ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "        rms_fill=rms_x#0.15\n",
    "        ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                    alpha=0.5, label=\"1 standard deviation\")\n",
    "        ax1.set_xlim(-4, 4);\n",
    "        ax1.set_ylim(-4, 4);\n",
    "        ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "        ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "        ax1.set_title(model_name+\" Model Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "        #hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "        textstr = '\\n'.join((\n",
    "            r'Raw scatter=%.3f m/s' % (sd_x, ),\n",
    "            r'Corrected scatter=%.3f m/s' % (rms_x, ),\n",
    "            r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "        ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "            ha=\"left\", va=\"top\",\n",
    "            bbox=dict(boxstyle=\"square\",\n",
    "                      ec=(1., 0.5, 0.5),\n",
    "                      fc=(1., 0.8, 0.8),\n",
    "                      ))\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "    else:\n",
    "        textstr = '\\n'.join((\n",
    "            r'Raw scatter=%.3f m/s' % (sd_x, ),\n",
    "            r'Corrected scatter=%.3f m/s' % (rms_x, ),\n",
    "            r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "        print(textstr)\n",
    "    \n",
    "    return metric_values\n",
    "  \n",
    "    ###########################\n",
    "    ### fix code below here ###\n",
    "    ###########################\n",
    "    \n",
    "def train_no_logging(model, hparams, plots, model_name, num_epochs, TRAIN_FILE_NAME, VAL_FILE_NAME):#=100):\n",
    "    train_dataset = load_dataset([TRAIN_FILE_NAME], batch_size=hparams.batch_size, mode=tf.estimator.ModeKeys.TRAIN)\n",
    "    val_dataset = load_dataset([VAL_FILE_NAME], batch_size=min(1024, NUM_VALIDATION_EXAMPLES), mode=tf.estimator.ModeKeys.EVAL)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    Opt = tfa.optimizers.extend_with_decoupled_weight_decay(tf.optimizers.SGD)\n",
    "    optimizer = Opt(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate, momentum=hparams.momentum)\n",
    "    metrics = [\n",
    "             tf.keras.metrics.MeanSquaredError(\"train_loss\"),\n",
    "             tf.keras.metrics.RootMeanSquaredError(\"train_rmse\")\n",
    "    ]\n",
    "    weight_decay_list_t.append(hparams.weight_decay)\n",
    "    gaussian_noise_list_t.append(hparams.gaussian_noise_scale)\n",
    "\n",
    "    metric_values = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Reset metric values for each new epoch.\n",
    "        for m in metrics:\n",
    "            m.reset_states()\n",
    "\n",
    "        # Train over all batches in the training set.\n",
    "        for features, labels, bjds in train_dataset:\n",
    "            if hparams.gaussian_noise_scale:\n",
    "                features += tf.random.normal(features.shape, stddev=hparams.gaussian_noise_scale)\n",
    "                #print(hparams.gaussian_noise_scale)\n",
    "            # One training step.\n",
    "            with tf.GradientTape() as t:\n",
    "                preds = model(features, training=True)\n",
    "                loss = loss_fn(labels, preds)\n",
    "            grads = t.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            # Update the metrics.\n",
    "            for m in metrics:\n",
    "                m(labels, preds)\n",
    "    \n",
    "        # End of an epoch.\n",
    "        epoch_metrics = {\"epoch\": epoch}\n",
    "        # First, log the training metrics.\n",
    "        for m in metrics:\n",
    "            epoch_metrics[m.name] = m.result().numpy()\n",
    "        # Next, evaluate over the validation set.\n",
    "        labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "        epoch_metrics[\"val_rmse\"] = np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "        # Add a metric for the raw scatter started with\n",
    "        epoch_metrics[\"original_rmse\"] = np.std(labels_val)\n",
    "        # Add a metric for raw scatter - corrected scatter\n",
    "        epoch_metrics[\"difference_rmse\"] = np.std(labels_val) - np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "        # Log metrics to tensorboard.\n",
    "        for metric, value in epoch_metrics.items():\n",
    "            tf.summary.scalar(metric, value, step=epoch)\n",
    "        epoch_metrics[\"epoch\"] = epoch\n",
    "        # Print metric values at selected epochs.\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            print(\"{epoch}: Train loss: {train_loss:.4}, Train RMSE: {train_rmse:.4}, Val RMSE: {val_rmse:.4}\".format(**epoch_metrics))\n",
    "        metric_values.append(epoch_metrics)\n",
    "\n",
    "    # Gather predictions\n",
    "    labels, preds, bjd = make_predictions(model, train_dataset)\n",
    "    labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "    all_bjds_val.append(bjd_val)\n",
    "    bjd_run_val.append(bjd_val)\n",
    "    all_pred_val.append(preds_val)\n",
    "    pred_run_val.append(preds_val)\n",
    "    all_labels_val.append(labels_val)\n",
    "    labels_run_val.append(labels_val)\n",
    "\n",
    "    # Scatter reduction plot\n",
    "    sd_x = np.std(labels_val, ddof=1)\n",
    "    rms_x = np.sqrt(np.mean(np.square(labels_val - preds_val)))\n",
    "    rms_x_list.append(rms_x)\n",
    "    rms_avg_list.append(rms_x)\n",
    "    stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "    x_range = np.linspace(-4,5.5, 17)\n",
    "    upper_bound = x_range+rms_x\n",
    "    lower_bound = x_range-rms_x\n",
    "\n",
    "    if plots==\"ON\":\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(24, 6))\n",
    "        ax = axes[0]\n",
    "        ax.plot([m[\"train_rmse\"] for m in metric_values], label=\"Train RMSE\")\n",
    "        ax.plot([m[\"val_rmse\"] for m in metric_values], label=\"Validation RMSE\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "        # Gather predictions to plot against labels.\n",
    "        ax = axes[1]\n",
    "        ax.plot(preds, labels, \".\", label=\"Training\")\n",
    "        ax.plot(preds_val, labels_val, \".\", label=\"Validation\")\n",
    "        ax.set_xlabel(\"Actual Y\")\n",
    "        ax.set_ylabel(\"Predicted Y\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # plot the scatter reduction plot\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        ax1.plot(labels_val, preds_val, \".\")\n",
    "        ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "        #ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "        rms_fill=rms_x#0.15\n",
    "        ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                        alpha=0.5, label=\"1 standard deviation\")\n",
    "        ax1.set_xlim(-4, 4);\n",
    "        ax1.set_ylim(-4, 4);\n",
    "        ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "        ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "        ax1.set_title(model_name+\" Model Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "        #hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "        textstr = '\\n'.join((\n",
    "            r'Raw scatter=%.3f m/s' % (sd_x, ),\n",
    "            r'Corrected scatter=%.3f m/s' % (rms_x, ),\n",
    "            r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "        ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "                ha=\"left\", va=\"top\",\n",
    "                bbox=dict(boxstyle=\"square\",\n",
    "                          ec=(1., 0.5, 0.5),\n",
    "                          fc=(1., 0.8, 0.8),\n",
    "                          ))\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "    else:\n",
    "        textstr = '\\n'.join((\n",
    "            r'Raw scatter=%.3f m/s' % (sd_x, ),\n",
    "            r'Corrected scatter=%.3f m/s' % (rms_x, ),\n",
    "            r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "        print(textstr)\n",
    "    \n",
    "    return metric_values\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8df3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for cross-val (v2)\n",
    "def make_predictions(model, dataset):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_bjds = []\n",
    "    for features, labels, bjds in dataset:\n",
    "        preds = model(features, training=False)\n",
    "        all_preds.append(preds.numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        all_bjds.append(bjds.numpy())\n",
    "    return np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_bjds)\n",
    "\n",
    "def train_cross_val(model, hparams, plots, model_name, num_epochs):#=100):\n",
    "    metrics_list = []\n",
    "    bjd_run_val = []\n",
    "    pred_run_val = []\n",
    "    labels_run_val = []\n",
    "    for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "        print(\"Model {}. Learning_rate: {:.5f}. Dense units: {}. Dense layers {}. Weight decay: {:.5f}\".format(\n",
    "          model_num, hparams.learning_rate, hparams.num_dense_units, hparams.num_dense_layers, hparams.weight_decay))\n",
    "        print(\"Cross-val number: \"+str(index+1))\n",
    "\n",
    "        TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "        VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "        print(TRAIN_FILE_NAME)\n",
    "        print(VAL_FILE_NAME)\n",
    "        metrics = [\n",
    "             tf.keras.metrics.MeanSquaredError(\"train_loss\"),\n",
    "             tf.keras.metrics.RootMeanSquaredError(\"train_rmse\")\n",
    "        ]\n",
    "\n",
    "        metric_values = train_no_logging(model, hparams, plots, model_name, num_epochs, TRAIN_FILE_NAME, VAL_FILE_NAME)\n",
    "        metrics_list.append(metric_values[-1])\n",
    "\n",
    "    crossval_metrics = {\"index\": index}\n",
    "    # compute median training metrices\n",
    "    for m in metrics:\n",
    "        crossval_metrics[m.name] = np.median([z[m.name] for z in metrics_list])\n",
    "    # compute median val, original and difference rmse\n",
    "    crossval_metrics['val_rmse'] = np.mean([z['val_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['original_rmse'] = np.mean([z['original_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['difference_rmse'] = np.mean([z['difference_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['epoch'] = np.mean([z['epoch'] for z in metrics_list])\n",
    "    print(\"overall val_rmse: \"+str(crossval_metrics['val_rmse']))\n",
    "    print(\"_____________________________________________________\")\n",
    "    # Log metrics to tensorboard.\n",
    "    for metric, value in crossval_metrics.items():\n",
    "        tf.summary.scalar(metric, value, step=crossval_metrics['epoch']) \n",
    "\n",
    "    mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "    mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "    mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "\n",
    "    all_mean_val_preds.append(mean_val_preds)\n",
    "    all_mean_val_labels.append(mean_val_labels)\n",
    "    all_mean_val_bjds.append(mean_val_bjds)\n",
    "\n",
    "    all_pred_val.append(pred_run_val)\n",
    "    all_labels_val.append(labels_run_val)\n",
    "    all_bjds_val.append(bjd_run_val)\n",
    "\n",
    "    return crossval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for cross-val (v1)\n",
    "def make_predictions(model, dataset):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_bjds = []\n",
    "    for features, labels, bjds in dataset:\n",
    "        preds = model(features, training=False)\n",
    "        all_preds.append(preds.numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        all_bjds.append(bjds.numpy())\n",
    "    return np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_bjds)\n",
    "\n",
    "def train_cross_val(model, hparams, plots, model_name, num_epochs):#=100):\n",
    "    metrics_list = []\n",
    "    for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "        print(\"Model {}. Learning_rate: {:.5f}. Dense units: {}. Dense layers {}. Weight decay: {:.5f}\".format(\n",
    "            model_num, hparams.learning_rate, hparams.num_dense_units, hparams.num_dense_layers, hparams.weight_decay))\n",
    "        print(\"Cross-val number: \"+str(index+1))\n",
    "\n",
    "        TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "        VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "        train_dataset = load_dataset([TRAIN_FILE_NAME], batch_size=hparams.batch_size, mode=tf.estimator.ModeKeys.TRAIN)\n",
    "        val_dataset = load_dataset([VAL_FILE_NAME], batch_size=min(1024, NUM_VALIDATION_EXAMPLES), mode=tf.estimator.ModeKeys.EVAL)\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "        Opt = tfa.optimizers.extend_with_decoupled_weight_decay(tf.optimizers.SGD)\n",
    "        optimizer = Opt(weight_decay=hparams.weight_decay, learning_rate=hparams.learning_rate, momentum=hparams.momentum)\n",
    "        metrics = [\n",
    "              tf.keras.metrics.MeanSquaredError(\"train_loss\"),\n",
    "              tf.keras.metrics.RootMeanSquaredError(\"train_rmse\")\n",
    "        ]\n",
    "        weight_decay_list_t.append(hparams.weight_decay)\n",
    "        gaussian_noise_list_t.append(hparams.gaussian_noise_scale)\n",
    "\n",
    "        # Reset metric values for each new cross-val slice.\n",
    "        for m in metrics:\n",
    "            m.reset_states()\n",
    "\n",
    "        metric_values = []\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            # Reset metric values for each new epoch.\n",
    "            for m in metrics:\n",
    "                m.reset_states()\n",
    "\n",
    "            # Train over all batches in the training set.\n",
    "            for features, labels, bjds in train_dataset:\n",
    "                if hparams.gaussian_noise_scale:\n",
    "                    features += tf.random.normal(features.shape, stddev=hparams.gaussian_noise_scale)\n",
    "                    #print(hparams.gaussian_noise_scale)\n",
    "                # One training step.\n",
    "                with tf.GradientTape() as t:\n",
    "                    preds = model(features, training=True)\n",
    "                    loss = loss_fn(labels, preds)\n",
    "                grads = t.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "                # Update the metrics.\n",
    "                for m in metrics:\n",
    "                    m(labels, preds)\n",
    "      \n",
    "            # End of an epoch.\n",
    "            epoch_metrics = {\"epoch\": epoch}\n",
    "            # First, log the training metrics.\n",
    "            for m in metrics:\n",
    "                epoch_metrics[m.name] = m.result().numpy()\n",
    "            # Next, evaluate over the validation set.\n",
    "            labels_val, preds_val, bjd_val = make_predictions(model, val_dataset)\n",
    "            epoch_metrics[\"val_rmse\"] = np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "            # Add a metric for the raw scatter started with\n",
    "            epoch_metrics[\"original_rmse\"] = np.std(labels_val)\n",
    "            # Add a metric for raw scatter - corrected scatter\n",
    "            epoch_metrics[\"difference_rmse\"] = np.std(labels_val) - np.sqrt(np.mean(np.square(preds_val - labels_val)))\n",
    "            # Log metrics to tensorboard.\n",
    "            #for metric, value in epoch_metrics.items():\n",
    "            #  tf.summary.scalar(metric, value, step=epoch)\n",
    "            epoch_metrics[\"epoch\"] = epoch\n",
    "            # Print metric values at selected epochs.\n",
    "            if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "                print(\"{epoch}: Train loss: {train_loss:.4}, Train RMSE: {train_rmse:.4}, Val RMSE: {val_rmse:.4}\".format(**epoch_metrics))\n",
    "            metric_values.append(epoch_metrics)\n",
    "        metrics_list.append(metric_values[-1])\n",
    "\n",
    "    crossval_metrics = {\"index\": index}\n",
    "    # compute median training metrices\n",
    "    for m in metrics:\n",
    "        crossval_metrics[m.name] = np.median([z[m.name] for z in metrics_list])\n",
    "    # compute median val, original and difference rmse\n",
    "    crossval_metrics['val_rmse'] = np.mean([z['val_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['original_rmse'] = np.median([z['original_rmse'] for z in metrics_list])\n",
    "    crossval_metrics['difference_rmse'] = np.median([z['difference_rmse'] for z in metrics_list])\n",
    "    # Log metrics to tensorboard.\n",
    "    for metric, value in crossval_metrics.items():\n",
    "        tf.summary.scalar(metric, value, step=epoch) \n",
    "\n",
    "    return crossval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0099a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear NN Model run\n",
    "\n",
    "# Cross validation Linear NN Model Run (10 times)\n",
    "\n",
    "\n",
    "rms_avg_list = []\n",
    "weight_decay_list_t = []\n",
    "gaussian_noise_list_t = []\n",
    "rms_x_list = []\n",
    "\n",
    "all_bjds_val = []\n",
    "all_pred_val = []\n",
    "all_labels_val = []\n",
    "all_mean_val_preds = []\n",
    "all_mean_val_bjds = []\n",
    "all_mean_val_labels = []\n",
    "all_mean_val_bjds = []\n",
    "\n",
    "for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "    TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "    VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "    bjd_run_val = []\n",
    "    pred_run_val = []\n",
    "    labels_run_val = []\n",
    "    all_bjds_val = []\n",
    "    for k in range(0,1):#10): #should be 10 for a full run\n",
    "        hparams = configdict.ConfigDict(dict(\n",
    "          num_features=401,\n",
    "          learning_rate=1e-3,\n",
    "          momentum=0.9,\n",
    "          batch_size=1024,\n",
    "          weight_decay=1e-5, \n",
    "          gaussian_noise_scale=0, \n",
    "        ))\n",
    "        model = LinearModel(hparams)\n",
    "        train(model, hparams, plots=\"ON\", model_name = \"Linear\", num_epochs=45)\n",
    "        print(model)\n",
    "        print(\"________________________\")\n",
    "        print(\"Cross-val number: \"+str(index+1)+\", Run number: \"+str(k+1))\n",
    "\n",
    "    mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "    mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "    mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "    all_mean_val_preds.append(mean_val_preds.tolist())\n",
    "    all_mean_val_labels.append(mean_val_labels.tolist())\n",
    "    all_mean_val_bjds.append(mean_val_bjds.tolist())\n",
    "avg = np.mean(rms_avg_list)\n",
    "print(\"________________________\")\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(all_mean_val_labels[0],all_mean_val_preds[0], all_mean_val_bjds[0])), \n",
    "               columns =['labels', 'preds', 'BJD']) \n",
    "df_sorted = df.sort_values(by=['BJD'])\n",
    "#df_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/cross_val_preds/val_preds_linearNN_06_30_2021.xlsx', index = False)\n",
    "\n",
    "# calculation of scatter removed\n",
    "from astropy.stats import median_absolute_deviation\n",
    "\n",
    "#df_sorted = df_from_excel\n",
    "labels_68_percent_by_2 = (np.percentile(df_sorted[\"labels\"], 84)-np.percentile(df_sorted[\"labels\"], 16))/2\n",
    "labels_median_absolute_deviation_norm = median_absolute_deviation(df_sorted[\"labels\"])*1.4826\n",
    "preds_68_percent_by_2 = (np.percentile(df_sorted[\"preds\"], 84)-np.percentile(df_sorted[\"preds\"], 16))/2\n",
    "\n",
    "corrected_rvs = df_sorted[\"labels\"]-df_sorted[\"preds\"]\n",
    "corrected_rvs_68_percent_by_2 = (np.percentile(corrected_rvs, 84)-np.percentile(corrected_rvs, 16))/2\n",
    "corrected_rvs_median_absolute_deviation_norm = median_absolute_deviation(corrected_rvs)*1.4826\n",
    "\n",
    "sd_labels = np.std(df_sorted[\"labels\"], ddof=1)\n",
    "sd_corrected_rv = np.std(corrected_rvs, ddof=1)\n",
    "rms_x = np.sqrt(np.mean(np.square(df_sorted[\"labels\"] - df_sorted[\"preds\"])))\n",
    "\n",
    "\n",
    "print(\"labels: sd: \"+str(sd_labels))\n",
    "print(\"corrected rv: sd: \"+str(sd_corrected_rv))\n",
    "print(\"____________________________________________\")\n",
    "print(\"labels: MAD*1.4826: \"+str(labels_median_absolute_deviation_norm))\n",
    "print(\"corrected rv: MAD*1.4826: \"+str(corrected_rvs_median_absolute_deviation_norm))\n",
    "print(\"____________________________________________\")\n",
    "print(\"labels: 68 percent/2: \"+str(labels_68_percent_by_2))\n",
    "print(\"corrected rv: 68 percent/2: \"+str(corrected_rvs_68_percent_by_2))\n",
    "\n",
    "\n",
    "# plot the average results\n",
    "# Scatter reduction plot\n",
    "sd_x = np.std(all_mean_val_labels[0], ddof=1)\n",
    "rms_x = np.sqrt(np.mean(np.square(all_mean_val_labels[0] - np.array(all_mean_val_preds[0]))))\n",
    "rms_x_list.append(rms_x)\n",
    "rms_avg_list.append(rms_x)\n",
    "stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "x_range = np.linspace(-4,5.5, 17)\n",
    "upper_bound = x_range+rms_x\n",
    "lower_bound = x_range-rms_x\n",
    "\n",
    "# plot the scatter reduction plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(21, 6))\n",
    "ax1 = ax[0]\n",
    "ax1.plot(df_sorted[\"labels\"], df_sorted[\"preds\"], \".\")\n",
    "ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "#ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "rms_fill=rms_x#0.15\n",
    "ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                alpha=0.5, label=\"1 standard deviation\")\n",
    "ax1.set_xlim(-4, 5);\n",
    "ax1.set_ylim(-4, 5);\n",
    "ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "ax1.set_title(\"Linear NN Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "#hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "textstr = '\\n'.join((\n",
    "    r'Raw scatter=%.3f m/s' % (labels_68_percent_by_2, ),\n",
    "    r'Corrected scatter=%.3f m/s' % (corrected_rvs_68_percent_by_2, ),\n",
    "    r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "        ha=\"left\", va=\"top\",\n",
    "        bbox=dict(facecolor='#fdcf44',edgecolor='k',\n",
    "                  boxstyle=\"square\",\n",
    "                  #ec=(1., 0.5, 0.5),\n",
    "                  #fc=(1., 0.8, 0.8),\n",
    "                  ))\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# plot predictions over time\n",
    "ax2 = ax[1]\n",
    "ax2.plot(df_sorted[\"BJD\"], df_sorted[\"labels\"], \".\",color='k',\n",
    "         markersize=10,label=\"labels\")\n",
    "ax2.plot(df_sorted[\"BJD\"], df_sorted[\"preds\"], \".\", markersize=10,\n",
    "         label=\"preds\")\n",
    "ax2.set_xlabel(\"Time (BJD)\")\n",
    "ax2.set_ylabel(\"RV (m/s)\")\n",
    "ax2.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d319f25",
   "metadata": {},
   "source": [
    "## Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19005dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBuilder(object):\n",
    "    \"\"\"Dataset builder class.\"\"\"\n",
    "\n",
    "    def __init__(self, file_pattern, hparams, mode, ccf_len, repeat=1):\n",
    "        \"\"\"Initializes the dataset builder.\n",
    "        Args:\n",
    "          file_pattern: File pattern matching input file shards, e.g.\n",
    "            \"/tmp/train-?????-of-00100\".\n",
    "          hparams: A ConfigDict.\n",
    "          mode: A tf.estimator.ModeKeys.\n",
    "          repeat: The number of times to repeat the dataset. If None, the dataset\n",
    "            will repeat indefinitely.\n",
    "        \"\"\"\n",
    "        valid_modes = [\n",
    "            tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL,\n",
    "            tf.estimator.ModeKeys.PREDICT\n",
    "        ]\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(\"Expected mode in {}. Got: {}\".format(valid_modes, mode))\n",
    "\n",
    "        self.file_pattern = file_pattern\n",
    "        self.hparams = hparams\n",
    "        self.mode = mode\n",
    "        self.repeat = repeat\n",
    "        self.ccf_len = ccf_len\n",
    "\n",
    "    def __call__(self):\n",
    "        is_training = self.mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "        # Dataset of file names.\n",
    "        filename_dataset = tf.data.Dataset.list_files(self.file_pattern,\n",
    "                                                      shuffle=is_training)\n",
    "\n",
    "        # Dataset of serialized tf.Examples.\n",
    "        dataset = filename_dataset.flat_map(tf.data.TFRecordDataset)\n",
    "\n",
    "        # Shuffle in training mode.\n",
    "        if is_training:\n",
    "            dataset = dataset.shuffle(self.hparams.shuffle_values_buffer)\n",
    "\n",
    "        # Possibly repeat.\n",
    "        if self.repeat != 1:\n",
    "            dataset = dataset.repeat(self.repeat)\n",
    "\n",
    "        def _example_parser(serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into feature and label tensors.\"\"\"\n",
    "            data_fields = {\n",
    "                self.hparams.ccf_feature_name: tf.io.FixedLenFeature([self.ccf_len], tf.float32),\n",
    "                self.hparams.label_feature_name: tf.io.FixedLenFeature([], tf.float32),\n",
    "                self.hparams.label_feature_name2: tf.io.FixedLenFeature([], tf.float32),\n",
    "            }\n",
    "            parsed_fields = tf.io.parse_single_example(serialized_example, features=data_fields)\n",
    "            ccf_data = parsed_fields[self.hparams.ccf_feature_name]\n",
    "            label = parsed_fields[self.hparams.label_feature_name]\n",
    "            label *= self.hparams.label_rescale_factor  # Rescale the label.\n",
    "            label2 = parsed_fields[self.hparams.label_feature_name2]\n",
    "            return {\n",
    "                \"ccf_data\": ccf_data,\n",
    "                \"label\": label,\n",
    "                \"bjd\": label2,\n",
    "            }\n",
    "\n",
    "        # Map the parser over the dataset.\n",
    "        dataset = dataset.map(_example_parser, num_parallel_calls=4)\n",
    "\n",
    "        # Batch results by up to batch_size.\n",
    "        dataset = dataset.batch(self.hparams.batch_size)\n",
    "\n",
    "        # Prefetch a few batches.\n",
    "        dataset = dataset.prefetch(10)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c24534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_ridge(filename, ccf_len):\n",
    "    dataset_hparams = configdict.ConfigDict(dict(\n",
    "        ccf_feature_name='Rescaled CCF_residuals_cutoff',#\"Rescaled CCF_residuals\", #CCF_residuals\n",
    "        label_feature_name= \"activity signal\",#\"RV\",\n",
    "        label_feature_name2= \"BJD\",\n",
    "        batch_size=300,\n",
    "        label_rescale_factor=1000,\n",
    "      ))\n",
    "    dataset = DatasetBuilder(filename, dataset_hparams, tf.estimator.ModeKeys.EVAL, ccf_len)()\n",
    "    batches = list(dataset)\n",
    "    ccf_data, labels, bjds = zip(*[(batch[\"ccf_data\"], batch[\"label\"], batch[\"bjd\"]) for batch in batches])\n",
    "    ccf_data = np.concatenate(ccf_data)\n",
    "    labels = np.concatenate(labels)\n",
    "    bjds = np.concatenate(bjds)\n",
    "    assert len(ccf_data.shape) == 2\n",
    "    assert len(labels.shape) == 1\n",
    "    assert len(bjds.shape) == 1\n",
    "    assert ccf_data.shape[0] == labels.shape[0]\n",
    "    #print(\"Read dataset with {} examples\".format(labels.shape[0]))\n",
    "    return ccf_data, labels, bjds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3551db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regress_harps(TRAIN_FILE_NAME_LIST, VAL_FILE_NAME_LIST, alpha, verbose, ccf_len):\n",
    "    rms_avg_list = []\n",
    "    weight_decay_list_t = []\n",
    "    gaussian_noise_list_t = []\n",
    "    rms_x_list = []\n",
    "\n",
    "    all_bjds_val = []\n",
    "    all_pred_val = []\n",
    "    all_labels_val = []\n",
    "    all_mean_val_preds = []\n",
    "    all_mean_val_bjds = []\n",
    "    all_mean_val_labels = []\n",
    "    all_mean_val_bjds = []\n",
    "    avg_list = []\n",
    "\n",
    "    for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "        TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "        VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "        train_X, train_Y, train_bjd  = load_dataset_ridge(TRAIN_FILE_NAME, ccf_len)\n",
    "        val_X, val_Y, val_bjd = load_dataset_ridge(VAL_FILE_NAME, ccf_len)\n",
    "\n",
    "        pred_run_val = []\n",
    "        labels_run_val = []\n",
    "        bjd_run_val = []\n",
    "        for k in range(0,10):\n",
    "            model = Ridge(alpha=alpha).fit(train_X, train_Y)\n",
    "            val_pred_Y = model.predict(val_X)\n",
    "            pred_run_val.append(val_pred_Y)\n",
    "            labels_run_val.append(val_Y)\n",
    "            bjd_run_val.append(val_bjd)\n",
    "            rms_avg = np.sqrt(np.mean(np.square(val_Y -val_pred_Y)))\n",
    "            rms_avg_list.append(rms_avg)\n",
    "            if verbose == True:\n",
    "                print(model)\n",
    "                print(\"________________________\")\n",
    "                print(\"Cross-val number: \"+str(index+1)+\", Run number: \"+str(k+1))\n",
    "                print(\"rms: \"+str(rms_avg))\n",
    "            else:\n",
    "                continue\n",
    "        mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "        mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "        mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "        all_mean_val_preds.append(mean_val_preds.tolist())\n",
    "        all_mean_val_labels.append(mean_val_labels.tolist())\n",
    "        all_mean_val_bjds.append(mean_val_bjds.tolist())\n",
    "    avg = np.mean(rms_avg_list)\n",
    "    avg_list.append(avg)\n",
    "    print(\"________________________\")\n",
    "    print(\"average rms = \"+str(avg)+\" m/s\")\n",
    "\n",
    "    #flatten the lists\n",
    "    all_mean_val_preds = [item for sublist in all_mean_val_preds for item in sublist]\n",
    "    all_mean_val_labels = [item for sublist in all_mean_val_labels for item in sublist]\n",
    "    all_mean_val_bjds = [item for sublist in all_mean_val_bjds for item in sublist]\n",
    "  \n",
    "    return all_mean_val_preds, all_mean_val_labels, all_mean_val_bjds, avg_list, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f42b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation Ridge Regression Model Run (10 times)\n",
    "\n",
    "all_mean_val_preds, all_mean_val_labels, all_mean_val_bjds, avg_rms_list, alpha = ridge_regress_harps(TRAIN_FILE_NAME_LIST, \n",
    "                                                                                 VAL_FILE_NAME_LIST, \n",
    "                                                                                 alpha=3.6094,#340.010636,#9.469, #340.010636,\t#335.734079,#9.469, \n",
    "                                                                                 verbose=False,\n",
    "                                                                                 ccf_len = 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(all_mean_val_labels,all_mean_val_preds, all_mean_val_bjds)), \n",
    "               columns =['labels', 'preds', 'BJD']) \n",
    "df_sorted = df.sort_values(by=['BJD'])\n",
    "#df_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/cross_val_preds/val_preds_ridge_06_30_2021.xlsx', index = False)\n",
    "\n",
    "# calculation of scatter removed\n",
    "from astropy.stats import median_absolute_deviation\n",
    "\n",
    "#df_sorted = df_from_excel\n",
    "labels_68_percent_by_2 = (np.percentile(df_sorted[\"labels\"], 84)-np.percentile(df_sorted[\"labels\"], 16))/2\n",
    "labels_median_absolute_deviation_norm = median_absolute_deviation(df_sorted[\"labels\"])*1.4826\n",
    "preds_68_percent_by_2 = (np.percentile(df_sorted[\"preds\"], 84)-np.percentile(df_sorted[\"preds\"], 16))/2\n",
    "\n",
    "corrected_rvs = df_sorted[\"labels\"]-df_sorted[\"preds\"]\n",
    "corrected_rvs_68_percent_by_2 = (np.percentile(corrected_rvs, 84)-np.percentile(corrected_rvs, 16))/2\n",
    "corrected_rvs_median_absolute_deviation_norm = median_absolute_deviation(corrected_rvs)*1.4826\n",
    "\n",
    "sd_labels = np.std(df_sorted[\"labels\"], ddof=1)\n",
    "sd_corrected_rv = np.std(corrected_rvs, ddof=1)\n",
    "rms_x = np.sqrt(np.mean(np.square(df_sorted[\"labels\"] - df_sorted[\"preds\"])))\n",
    "\n",
    "\n",
    "print(\"labels: sd: \"+str(sd_labels))\n",
    "print(\"corrected rv: sd: \"+str(sd_corrected_rv))\n",
    "print(\"____________________________________________\")\n",
    "print(\"labels: MAD*1.4826: \"+str(labels_median_absolute_deviation_norm))\n",
    "print(\"corrected rv: MAD*1.4826: \"+str(corrected_rvs_median_absolute_deviation_norm))\n",
    "print(\"____________________________________________\")\n",
    "print(\"labels: 68 percent/2: \"+str(labels_68_percent_by_2))\n",
    "print(\"corrected rv: 68 percent/2: \"+str(corrected_rvs_68_percent_by_2))\n",
    "\n",
    "\n",
    "# plot the average results\n",
    "# Scatter reduction plot\n",
    "rms_avg_list = []\n",
    "sd_x = np.std(all_mean_val_labels, ddof=1)\n",
    "rms_x = np.sqrt(np.mean(np.square(all_mean_val_labels - np.array(all_mean_val_preds))))\n",
    "#rms_x_list.append(rms_x)\n",
    "rms_avg_list.append(rms_x)\n",
    "stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "x_range = np.linspace(-4,5.5, 17)\n",
    "upper_bound = x_range+rms_x\n",
    "lower_bound = x_range-rms_x\n",
    "\n",
    "# plot the scatter reduction plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(21, 6))\n",
    "ax1 = ax[0]\n",
    "ax1.plot(df_sorted[\"labels\"], df_sorted[\"preds\"], \".\")\n",
    "ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "#ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "rms_fill=rms_x#0.15\n",
    "ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                alpha=0.5, label=\"1 standard deviation\")\n",
    "ax1.set_xlim(-4, 5);\n",
    "ax1.set_ylim(-4, 5);\n",
    "ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "ax1.set_title(\"Ridge Regression Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "#hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "textstr = '\\n'.join((\n",
    "    r'Raw scatter=%.3f m/s' % (sd_labels, ),\n",
    "    r'Corrected scatter=%.3f m/s' % (sd_corrected_rv, ),\n",
    "    r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "ax1.text(-3.8, 4.5, textstr, size=15,\n",
    "        ha=\"left\", va=\"top\",\n",
    "        bbox=dict(facecolor='#fdcf44',edgecolor='k',\n",
    "                  boxstyle=\"square\",\n",
    "                  #ec=(1., 0.5, 0.5),\n",
    "                  #fc=(1., 0.8, 0.8),\n",
    "                  ))\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# plot predictions over time\n",
    "ax2 = ax[1]\n",
    "ax2.plot(df_sorted[\"BJD\"], df_sorted[\"labels\"], \".\",color='k',\n",
    "         markersize=10,label=\"labels\")\n",
    "ax2.plot(df_sorted[\"BJD\"], df_sorted[\"preds\"], \".\", markersize=10,\n",
    "         label=\"preds\")\n",
    "ax2.set_xlabel(\"Time (BJD)\")\n",
    "ax2.set_ylabel(\"RV (m/s)\")\n",
    "ax2.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "train_X, train_Y, train_bjd  = load_dataset_ridge(TRAIN_FILE_NAME, ccf_len)\n",
    "val_X, val_Y, val_bjd = load_dataset_ridge(VAL_FILE_NAME, ccf_len)\n",
    "model = Ridge(alpha=alpha).fit(train_X, train_Y)\n",
    "plt.plot(model.coef_, \".\")\n",
    "\n",
    "residual_plot(train_Y, np.linspace(-19,19,ccf_len), train_X, \"median\",\"Old Test set: Residual CCFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot old dataset\n",
    "residual_plot(val_Y, np.linspace(-20,20,ccf_len), val_X, \"median\",\"Old Test set: Residual CCFs\")\n",
    "\n",
    "df_old = pd.DataFrame(list(zip(val_Y,val_X, val_bjd)), columns =['val_Y', 'val_X', 'BJD']) \n",
    "df_old_sorted = df_old.sort_values(by=['BJD'])\n",
    "df_old_sorted = df_old_sorted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ba491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha cross-val runs\n",
    "\n",
    "# Cross validation Ridge Regression Model Run (10 times)\n",
    "alpha_range = np.random.uniform(2, 5.5, 100)\n",
    "#alpha_range = np.append(alpha_range, 9.469)\n",
    "\n",
    "alpha_list = []\n",
    "rms_crossval_list = []\n",
    "for alpha in tqdm(alpha_range):\n",
    "    all_mean_val_preds, all_mean_val_labels, all_mean_val_bjds, avg_rms_list, alpha = ridge_regress_harps(TRAIN_FILE_NAME_LIST, \n",
    "                                                                                  VAL_FILE_NAME_LIST, \n",
    "                                                                                  alpha=alpha, \n",
    "                                                                                  verbose=False,\n",
    "                                                                                  ccf_len= 46)\n",
    "    # compute rms across all crossval slice\n",
    "    rms_crossval = np.mean(avg_rms_list)\n",
    "    rms_crossval_list.append(rms_crossval)\n",
    "    alpha_list.append(alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc93daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rms = pd.DataFrame(list(zip(alpha_list, rms_crossval_list)), \n",
    "               columns =['alpha_list', 'avg_rms_list'])\n",
    "df_rms_sorted = df_rms.sort_values(by=['avg_rms_list'])\n",
    "#df_rms_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/Clean_June28_rv_net_code/crossval_preds/alpha_rms_linear_12_6.xlsx', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80500794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the files\n",
    "df_rms_sorted_1 = pd.read_excel('gdrive/Shared drives/Exoplanet_RV/Clean_June28_rv_net_code/crossval_preds/alpha_rms_linear_12_1.xlsx')\n",
    "df_rms_sorted_2 = pd.read_excel('gdrive/Shared drives/Exoplanet_RV/Clean_June28_rv_net_code/crossval_preds/alpha_rms_linear_12_1_v2.xlsx')\n",
    "\n",
    "# Combine dataframes\n",
    "frames = [df_rms_sorted_1, df_rms_sorted_2]\n",
    "df_rms_sorted = pd.concat(frames)\n",
    "\n",
    "df_rms_sorted = df_rms_sorted.sort_values(by=['avg_rms_list'])\n",
    "df_rms_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eaaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "plt.plot(df_rms_sorted[\"alpha_list\"],df_rms_sorted[\"avg_rms_list\"], \".\", color='purple',\n",
    "         markersize=9)\n",
    "plt.title(\"Linear Model Performance as a Function of Alpha\")\n",
    "plt.xlabel(\"Alpha Value\")\n",
    "plt.ylabel(\"RMS (m/s)\")\n",
    "#plt.xlim(0, 20)\n",
    "#plt.ylim(0.94, 0.95)\n",
    "\n",
    "df_rms_sorted[\"alpha_list\"][np.argmin(np.array(df_rms_sorted[\"avg_rms_list\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210a270",
   "metadata": {},
   "source": [
    "## FC NN Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC NN Model run\n",
    "\n",
    "# Cross validation Linear NN Model Run (10 times)\n",
    "rms_avg_list = []\n",
    "weight_decay_list_t = []\n",
    "gaussian_noise_list_t = []\n",
    "rms_x_list = []\n",
    "\n",
    "all_bjds_val = []\n",
    "all_pred_val = []\n",
    "all_labels_val = []\n",
    "all_mean_val_preds = []\n",
    "all_mean_val_bjds = []\n",
    "all_mean_val_labels = []\n",
    "all_mean_val_bjds = []\n",
    "\n",
    "for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "    TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "    VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "    bjd_run_val = []\n",
    "    pred_run_val = []\n",
    "    labels_run_val = []\n",
    "    for k in range(0,1):#0): #should be 10 for a full run\n",
    "        hparams = configdict.ConfigDict(dict(\n",
    "          num_features=152,#161,\n",
    "          learning_rate=0.0054042, #0.040267, #0.0095352,#0.0016077,\n",
    "          momentum=0.9,\n",
    "          batch_size=300,\n",
    "          num_dense_units=100,#1000,#200,\n",
    "          num_dense_layers=8, #4,\n",
    "          weight_decay=0.00010000,#0.008000,#0.00010000, #5e-4, #7e-2,\n",
    "          gaussian_noise_scale=0,#1.5, \n",
    "        ))\n",
    "        model = FCModel(hparams)\n",
    "        train(model, hparams, plots=\"ON\",model_name=\"FC NN\", num_epochs=50)\n",
    "        print(model)\n",
    "        print(\"________________________\")\n",
    "        print(\"Cross-val number: \"+str(index+1)+\", Run number: \"+str(k+1))\n",
    "    mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "    mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "    mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "    all_mean_val_preds.append(mean_val_preds.tolist())\n",
    "    all_mean_val_labels.append(mean_val_labels.tolist())\n",
    "    all_mean_val_bjds.append(mean_val_bjds.tolist())\n",
    "avg = np.mean(rms_avg_list)\n",
    "print(\"________________________\")\n",
    "print(avg)\n",
    "\n",
    "#flatten the lists\n",
    "all_mean_val_preds = [item for sublist in all_mean_val_preds for item in sublist]\n",
    "all_mean_val_labels = [item for sublist in all_mean_val_labels for item in sublist]\n",
    "all_mean_val_bjds = [item for sublist in all_mean_val_bjds for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c365bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(all_mean_val_labels,all_mean_val_preds, all_mean_val_bjds)), \n",
    "               columns =['labels', 'preds', 'BJD']) \n",
    "df_sorted = df.sort_values(by=['BJD'])\n",
    "#df_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/cross_val_preds/val_preds_FCNN_06_30_2021.xlsx', index = False)\n",
    "\n",
    "# calculation of scatter removed\n",
    "from astropy.stats import median_absolute_deviation\n",
    "\n",
    "#df_sorted = df_from_excel\n",
    "labels_68_percent_by_2 = (np.percentile(df_sorted[\"labels\"], 84)-np.percentile(df_sorted[\"labels\"], 16))/2\n",
    "labels_median_absolute_deviation_norm = median_absolute_deviation(df_sorted[\"labels\"])*1.4826\n",
    "preds_68_percent_by_2 = (np.percentile(df_sorted[\"preds\"], 84)-np.percentile(df_sorted[\"preds\"], 16))/2\n",
    "\n",
    "corrected_rvs = df_sorted[\"labels\"]-df_sorted[\"preds\"]\n",
    "corrected_rvs_68_percent_by_2 = (np.percentile(corrected_rvs, 84)-np.percentile(corrected_rvs, 16))/2\n",
    "corrected_rvs_median_absolute_deviation_norm = median_absolute_deviation(corrected_rvs)*1.4826\n",
    "\n",
    "sd_labels = np.std(df_sorted[\"labels\"], ddof=1)\n",
    "sd_corrected_rv = np.std(corrected_rvs, ddof=1)\n",
    "rms_x = np.sqrt(np.mean(np.square(df_sorted[\"labels\"] - df_sorted[\"preds\"])))\n",
    "\n",
    "\n",
    "print(\"labels: sd: \"+str(sd_labels))\n",
    "print(\"corrected rv: sd: \"+str(sd_corrected_rv))\n",
    "print(\"____________________________________________\")\n",
    "print(\"labels: MAD*1.4826: \"+str(labels_median_absolute_deviation_norm))\n",
    "print(\"corrected rv: MAD*1.4826: \"+str(corrected_rvs_median_absolute_deviation_norm))\n",
    "print(\"____________________________________________\")\n",
    "print(\"labels: 68 percent/2: \"+str(labels_68_percent_by_2))\n",
    "print(\"corrected rv: 68 percent/2: \"+str(corrected_rvs_68_percent_by_2))\n",
    "\n",
    "\n",
    "# plot the average results\n",
    "# Scatter reduction plot\n",
    "rms_avg_list = []\n",
    "sd_x = np.std(all_mean_val_labels, ddof=1)\n",
    "rms_x = np.sqrt(np.mean(np.square(all_mean_val_labels - np.array(all_mean_val_preds))))\n",
    "#rms_x_list.append(rms_x)\n",
    "rms_avg_list.append(rms_x)\n",
    "stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "x_range = np.linspace(-4,5.5, 17)\n",
    "upper_bound = x_range+rms_x\n",
    "lower_bound = x_range-rms_x\n",
    "\n",
    "# plot the scatter reduction plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(21, 6))\n",
    "ax1 = ax[0]\n",
    "ax1.plot(df_sorted[\"labels\"], df_sorted[\"preds\"], \".\")\n",
    "ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "#ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "rms_fill=rms_x#0.15\n",
    "ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                alpha=0.5, label=\"1 standard deviation\")\n",
    "ax1.set_xlim(-4, 5);\n",
    "ax1.set_ylim(-4, 5);\n",
    "ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "ax1.set_title(\"FC NN Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "#hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "textstr = '\\n'.join((\n",
    "    r'Raw scatter=%.3f m/s' % (labels_68_percent_by_2, ),\n",
    "    r'Corrected scatter=%.3f m/s' % (corrected_rvs_68_percent_by_2, ),\n",
    "    r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "        ha=\"left\", va=\"top\",\n",
    "        bbox=dict(facecolor='#fdcf44',edgecolor='k',\n",
    "                  boxstyle=\"square\",\n",
    "                  #ec=(1., 0.5, 0.5),\n",
    "                  #fc=(1., 0.8, 0.8),\n",
    "                  ))\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# plot predictions over time\n",
    "ax2 = ax[1]\n",
    "ax2.plot(df_sorted[\"BJD\"], df_sorted[\"labels\"], \".\",color='k',\n",
    "         markersize=10,label=\"labels\")\n",
    "ax2.plot(df_sorted[\"BJD\"], df_sorted[\"preds\"], \".\", markersize=10,\n",
    "         label=\"preds\")\n",
    "ax2.set_xlabel(\"Time (BJD)\")\n",
    "ax2.set_ylabel(\"RV (m/s)\")\n",
    "ax2.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9f087e",
   "metadata": {},
   "source": [
    "## CNN Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model run\n",
    "\n",
    "# Cross validation CNN Model Run (10 times)\n",
    "rms_avg_list = []\n",
    "weight_decay_list_t = []\n",
    "gaussian_noise_list_t = []\n",
    "rms_x_list = []\n",
    "\n",
    "all_bjds_val = []\n",
    "all_pred_val = []\n",
    "all_labels_val = []\n",
    "all_mean_val_preds = []\n",
    "all_mean_val_bjds = []\n",
    "all_mean_val_labels = []\n",
    "all_mean_val_bjds = []\n",
    "\n",
    "for index in range(0, len(VAL_FILE_NAME_LIST)):\n",
    "    TRAIN_FILE_NAME = TRAIN_FILE_NAME_LIST[index]\n",
    "    VAL_FILE_NAME = VAL_FILE_NAME_LIST[index]\n",
    "    bjd_run_val = []\n",
    "    pred_run_val = []\n",
    "    labels_run_val = []\n",
    "    for k in range(0,2):#0): #should be 10 for a full run\n",
    "        hparams = configdict.ConfigDict(dict(\n",
    "            num_features=49,\n",
    "            learning_rate=0.0085099,#0.0050618, #1e-3,\n",
    "            momentum=0.9,\n",
    "            batch_size=300,\n",
    "            conv_kernel_size=3,\n",
    "            num_conv_filters=16, #32,\n",
    "            num_conv_layers=2,#4,\n",
    "            num_dense_units=100,#500,\n",
    "            num_dense_layers=1,\n",
    "            weight_decay=0.0017600,#0.0012341, #5e-4, #7e-2,\n",
    "            gaussian_noise_scale=0,#1.5,\n",
    "        ))\n",
    "        model = CNNModel(hparams)\n",
    "        train(model, hparams, plots=\"OFF\",model_name=\"CNN\", num_epochs=90)#35)#65)\n",
    "        print(model)\n",
    "        print(\"________________________\")\n",
    "        print(\"Cross-val number: \"+str(index+1)+\", Run number: \"+str(k+1))\n",
    "    mean_val_preds = np.mean(pred_run_val, axis=0)\n",
    "    mean_val_labels = np.mean(labels_run_val, axis=0)\n",
    "    mean_val_bjds = np.mean(bjd_run_val, axis=0)\n",
    "    all_mean_val_preds.append(mean_val_preds.tolist())\n",
    "    all_mean_val_labels.append(mean_val_labels.tolist())\n",
    "    all_mean_val_bjds.append(mean_val_bjds.tolist())\n",
    "\n",
    "avg = np.mean(rms_avg_list)\n",
    "print(\"________________________\")\n",
    "print(avg)\n",
    "\n",
    "#flatten the lists\n",
    "all_mean_val_preds = [item for sublist in all_mean_val_preds for item in sublist]\n",
    "all_mean_val_labels = [item for sublist in all_mean_val_labels for item in sublist]\n",
    "all_mean_val_bjds = [item for sublist in all_mean_val_bjds for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9efdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(all_mean_val_labels,all_mean_val_preds, all_mean_val_bjds)), \n",
    "               columns =['labels', 'preds', 'BJD']) \n",
    "df_sorted = df.sort_values(by=['BJD'])\n",
    "#df_sorted.to_excel('gdrive/Shared drives/Exoplanet_RV/cross_val_preds/val_preds_CNN_06_30_2021.xlsx', index = False)\n",
    "\n",
    "# calculation of scatter removed\n",
    "from astropy.stats import median_absolute_deviation\n",
    "\n",
    "#df_sorted = df_from_excel\n",
    "labels_68_percent_by_2 = (np.percentile(df_sorted[\"labels\"], 84)-np.percentile(df_sorted[\"labels\"], 16))/2\n",
    "labels_median_absolute_deviation_norm = median_absolute_deviation(df_sorted[\"labels\"])*1.4826\n",
    "preds_68_percent_by_2 = (np.percentile(df_sorted[\"preds\"], 84)-np.percentile(df_sorted[\"preds\"], 16))/2\n",
    "\n",
    "corrected_rvs = df_sorted[\"labels\"]-df_sorted[\"preds\"]\n",
    "corrected_rvs_68_percent_by_2 = (np.percentile(corrected_rvs, 84)-np.percentile(corrected_rvs, 16))/2\n",
    "corrected_rvs_median_absolute_deviation_norm = median_absolute_deviation(corrected_rvs)*1.4826\n",
    "\n",
    "sd_labels = np.std(df_sorted[\"labels\"], ddof=1)\n",
    "sd_corrected_rv = np.std(corrected_rvs, ddof=1)\n",
    "rms_x = np.sqrt(np.mean(np.square(df_sorted[\"labels\"] - df_sorted[\"preds\"])))\n",
    "\n",
    "\n",
    "print(\"labels: sd: \"+str(sd_labels))\n",
    "print(\"corrected rv: sd: \"+str(sd_corrected_rv))\n",
    "print(\"____________________________________________\")\n",
    "print(\"labels: MAD*1.4826: \"+str(labels_median_absolute_deviation_norm))\n",
    "print(\"corrected rv: MAD*1.4826: \"+str(corrected_rvs_median_absolute_deviation_norm))\n",
    "print(\"____________________________________________\")\n",
    "print(\"labels: 68 percent/2: \"+str(labels_68_percent_by_2))\n",
    "print(\"corrected rv: 68 percent/2: \"+str(corrected_rvs_68_percent_by_2))\n",
    "\n",
    "\n",
    "# plot the average results\n",
    "# Scatter reduction plot\n",
    "rms_avg_list = []\n",
    "sd_x = np.std(all_mean_val_labels, ddof=1)\n",
    "rms_x = np.sqrt(np.mean(np.square(all_mean_val_labels - np.array(all_mean_val_preds))))\n",
    "#rms_x_list.append(rms_x)\n",
    "rms_avg_list.append(rms_x)\n",
    "stel_removed = np.sqrt(np.abs(sd_x**2-rms_x**2))\n",
    "x_range = np.linspace(-4,5.5, 17)\n",
    "upper_bound = x_range+rms_x\n",
    "lower_bound = x_range-rms_x\n",
    "\n",
    "# plot the scatter reduction plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(21, 6))\n",
    "ax1 = ax[0]\n",
    "ax1.plot(df_sorted[\"labels\"], df_sorted[\"preds\"], \".\")\n",
    "ax1.plot(x_range,x_range, color=\"blue\", label=\"1:1 ratio\")\n",
    "#ax.plot(x_range,z[0]*x_range+z[1], color=\"blue\")\n",
    "rms_fill=rms_x#0.15\n",
    "ax1.fill_between(x_range, x_range+rms_fill, x_range-rms_fill, facecolor='lightblue',\n",
    "                alpha=0.5, label=\"1 standard deviation\")\n",
    "ax1.set_xlim(-4, 5);\n",
    "ax1.set_ylim(-4, 5);\n",
    "ax1.set_xlabel(\"HARPS-N Stellar Activity Signal (m/s)\", size =16)\n",
    "ax1.set_ylabel(\"Model Predicted Stellar Activity Signal (m/s)\", size =16)\n",
    "ax1.set_title(\"CNN Predictions of Stellar Activity signal(m/s)\") #, %d epochs, weight decay: %.2e, gauss noise: %.2e \" %(num_epochs, \n",
    "#hparams.weight_decay, hparams.gaussian_noise_scale, size=16)\n",
    "textstr = '\\n'.join((\n",
    "    r'Raw scatter=%.3f m/s' % (labels_68_percent_by_2, ),\n",
    "    r'Corrected scatter=%.3f m/s' % (corrected_rvs_68_percent_by_2, ),\n",
    "    r'Stellar Error Removed=%.3f m/s' % (stel_removed, )))\n",
    "ax1.text(-3.8, 3.5, textstr, size=15,\n",
    "        ha=\"left\", va=\"top\",\n",
    "        bbox=dict(facecolor='#fdcf44',edgecolor='k',\n",
    "                  boxstyle=\"square\",\n",
    "                  #ec=(1., 0.5, 0.5),\n",
    "                  #fc=(1., 0.8, 0.8),\n",
    "                  ))\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# plot predictions over time\n",
    "ax2 = ax[1]\n",
    "ax2.plot(df_sorted[\"BJD\"], df_sorted[\"labels\"], \".\",color='k',\n",
    "         markersize=10,label=\"labels\")\n",
    "ax2.plot(df_sorted[\"BJD\"], df_sorted[\"preds\"], \".\", markersize=10,\n",
    "         label=\"preds\")\n",
    "ax2.set_xlabel(\"Time (BJD)\")\n",
    "ax2.set_ylabel(\"RV (m/s)\")\n",
    "ax2.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248913a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
